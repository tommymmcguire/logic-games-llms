{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1UgHFY8v39Uf5uO9ROv3PgPMkG6w8vh-R",
      "authorship_tag": "ABX9TyOmGaBALeDPqiudTfQRd1e9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tommymmcguire/logic-games-llms/blob/main/notebooks/finetune_mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Lit GPT"
      ],
      "metadata": {
        "id": "vWzCJYzdfQdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'litgpt[all] @ git+https://github.com/Lightning-AI/litgpt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3FKcu3ZMqwS",
        "outputId": "307eb185-53f6-451a-83d2-204cf009114d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litgpt[all]@ git+https://github.com/Lightning-AI/litgpt\n",
            "  Cloning https://github.com/Lightning-AI/litgpt to /tmp/pip-install-1id3ku8n/litgpt_e9dc5ba220f24a188d201aa10693a6f4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Lightning-AI/litgpt /tmp/pip-install-1id3ku8n/litgpt_e9dc5ba220f24a188d201aa10693a6f4\n",
            "  Resolved https://github.com/Lightning-AI/litgpt to commit 502a9817bfbd025627c34c229b8c3eb2eca51150\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.2.1+cu121)\n",
            "Collecting lightning==2.3.0.dev20240318 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lightning-2.3.0.dev20240318-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonargparse[signatures]>=4.27.6 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading jsonargparse-4.27.6-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.2/192.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.42.0 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.1.99)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.15.2)\n",
            "Collecting datasets (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.31.0)\n",
            "Collecting litdata (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading litdata-0.2.2-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zstandard (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.5.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (14.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.15.2)\n",
            "Collecting torchmetrics (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.4.2)\n",
            "Collecting huggingface-hub[hf_transfer]>=0.21.0 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.42.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.11.4)\n",
            "Requirement already satisfied: PyYAML<6.1.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2023.11.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2023.6.0)\n",
            "Collecting lightning-utilities<0.11.0,>=0.8.0 (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.25.2)\n",
            "Collecting packaging<=23.1,>=20.0 (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch>=2.2.0 (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics (from litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.67.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.66.2)\n",
            "Collecting typing-extensions<4.10.0,>=4.4.0 (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Collecting pytorch-lightning (from lightning==2.3.0.dev20240318->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading pytorch_lightning-2.2.1-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.6/801.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[hf_transfer]>=0.21.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.13.1)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface-hub[hf_transfer]>=0.21.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docstring-parser>=0.15 (from jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading typeshed_client-2.5.1-py3-none-any.whl (606 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.1/606.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2024.2.2)\n",
            "Collecting lightning-cloud==0.5.64 (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading lightning_cloud-0.5.64-py3-none-any.whl (928 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m928.4/928.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.17.1+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (9.4.0)\n",
            "Collecting viztracer (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading viztracer-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3[crt] (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading boto3-1.34.65-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (8.1.7)\n",
            "Collecting fastapi (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.3.0)\n",
            "Collecting python-multipart (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (13.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.16.0)\n",
            "Collecting uvicorn (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading uvicorn-0.28.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2023.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]>=4.27.6->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.1.5)\n",
            "Collecting botocore<1.35.0,>=1.34.65 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading botocore-1.34.65-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.2.0->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objprint>0.1.3 (from viztracer->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading objprint-0.2.3-py3-none-any.whl (39 kB)\n",
            "Collecting awscrt==0.19.19 (from botocore<1.35.0,>=1.34.65->boto3[crt]->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading awscrt-0.19.19-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.2.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.6.4)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.16.1)\n",
            "Collecting h11>=0.8 (from uvicorn->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.37.0,>=0.36.3->fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (3.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.37.0,>=0.36.3->fastapi->lightning-cloud==0.5.64->litdata->litgpt[all]@ git+https://github.com/Lightning-AI/litgpt) (1.2.0)\n",
            "Building wheels for collected packages: litgpt\n",
            "  Building wheel for litgpt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for litgpt: filename=litgpt-0.3.0.dev0-py3-none-any.whl size=136723 sha256=a59108a1a8d25f683bf97e03afcc8a0eaabff5e34e519acb51d47c356ff7c25f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2q1ql92e/wheels/e1/35/84/dd27bf19233f92182da0cb9b138645406f0c7a780128f1beab\n",
            "Successfully built litgpt\n",
            "Installing collected packages: zstandard, xxhash, typing-extensions, python-multipart, packaging, objprint, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jsonargparse, jmespath, hf-transfer, h11, docstring-parser, dill, awscrt, viztracer, uvicorn, typeshed-client, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, lightning-utilities, huggingface-hub, botocore, bitsandbytes, s3transfer, nvidia-cusolver-cu12, torch, fastapi, datasets, boto3, torchvision, torchmetrics, lightning-cloud, pytorch-lightning, litdata, lightning, litgpt\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed awscrt-0.19.19 bitsandbytes-0.42.0 boto3-1.34.65 botocore-1.34.65 datasets-2.18.0 dill-0.3.8 docstring-parser-0.16 fastapi-0.110.0 h11-0.14.0 hf-transfer-0.1.6 huggingface-hub-0.21.4 jmespath-1.0.1 jsonargparse-4.27.6 lightning-2.3.0.dev20240318 lightning-cloud-0.5.64 lightning-utilities-0.10.1 litdata-0.2.2 litgpt-0.3.0.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 objprint-0.2.3 packaging-23.1 python-multipart-0.0.9 pytorch-lightning-2.2.1 s3transfer-0.10.1 starlette-0.36.3 torch-2.2.0 torchmetrics-1.2.1 torchvision-0.17.0 typeshed-client-2.5.1 typing-extensions-4.9.0 uvicorn-0.28.1 viztracer-0.16.2 xxhash-3.4.1 zstandard-0.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42B2BYEGfGip",
        "outputId": "19af2b0d-ada7-488a-9be7-99ba049cb543"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Model - Mistral7B"
      ],
      "metadata": {
        "id": "a1hOmj2wgrhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt download --repo_id mistralai/Mistral-7B-Instruct-v0.2\n",
        "!litgpt convert_hf_checkpoint --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyc47J5tfNNO",
        "outputId": "ded97482-5512-48d1-c895-a111abae687d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100% 596/596 [00:00<00:00, 4.26MB/s]\n",
            "generation_config.json: 100% 111/111 [00:00<00:00, 853kB/s]\n",
            "pytorch_model-00001-of-00003.bin: 100% 4.94G/4.94G [00:41<00:00, 118MB/s]\n",
            "pytorch_model-00002-of-00003.bin: 100% 5.00G/5.00G [00:43<00:00, 114MB/s] \n",
            "pytorch_model-00003-of-00003.bin: 100% 5.06G/5.06G [00:46<00:00, 109MB/s]\n",
            "pytorch_model.bin.index.json: 100% 23.9k/23.9k [00:00<00:00, 23.0MB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 29.5MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 422MB/s]\n",
            "tokenizer_config.json: 100% 1.46k/1.46k [00:00<00:00, 10.6MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "Processing checkpoints/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00001-of-00003.bin\n",
            "Loading 'model.embed_tokens.weight' into RAM\n",
            "Loading 'model.layers.0.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.0.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.0.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.1.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.1.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.1.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.2.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.2.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.2.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.3.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.3.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.3.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.4.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.4.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.4.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.5.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.5.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.5.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.6.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.6.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.6.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.6.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.7.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.7.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.7.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.7.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.8.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.8.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.8.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.8.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.9.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.9.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.9.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.9.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.10.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.10.mlp.up_proj.weight' into RAM\n",
            "Loading 'layer 0 q' into RAM\n",
            "Loading 'layer 0 k' into RAM\n",
            "Loading 'layer 0 v' into RAM\n",
            "Loading 'layer 1 q' into RAM\n",
            "Loading 'layer 1 k' into RAM\n",
            "Loading 'layer 1 v' into RAM\n",
            "Loading 'layer 2 q' into RAM\n",
            "Loading 'layer 2 k' into RAM\n",
            "Loading 'layer 2 v' into RAM\n",
            "Loading 'layer 3 q' into RAM\n",
            "Loading 'layer 3 k' into RAM\n",
            "Loading 'layer 3 v' into RAM\n",
            "Loading 'layer 4 q' into RAM\n",
            "Loading 'layer 4 k' into RAM\n",
            "Loading 'layer 4 v' into RAM\n",
            "Loading 'layer 5 q' into RAM\n",
            "Loading 'layer 5 k' into RAM\n",
            "Loading 'layer 5 v' into RAM\n",
            "Loading 'layer 6 q' into RAM\n",
            "Loading 'layer 6 k' into RAM\n",
            "Loading 'layer 6 v' into RAM\n",
            "Loading 'layer 7 q' into RAM\n",
            "Loading 'layer 7 k' into RAM\n",
            "Loading 'layer 7 v' into RAM\n",
            "Loading 'layer 8 q' into RAM\n",
            "Loading 'layer 8 k' into RAM\n",
            "Loading 'layer 8 v' into RAM\n",
            "Loading 'layer 9 q' into RAM\n",
            "Loading 'layer 9 k' into RAM\n",
            "Loading 'layer 9 v' into RAM\n",
            "Loading 'layer 10 q' into RAM\n",
            "Loading 'layer 10 k' into RAM\n",
            "Loading 'layer 10 v' into RAM\n",
            "Processing checkpoints/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00002-of-00003.bin\n",
            "Loading 'model.layers.10.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.10.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.10.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.11.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.11.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.11.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.11.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.12.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.12.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.12.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.12.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.13.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.13.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.13.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.13.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.14.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.14.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.14.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.14.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.15.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.15.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.15.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.15.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.16.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.16.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.16.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.16.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.17.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.17.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.17.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.17.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.18.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.18.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.18.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.18.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.19.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.19.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.19.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.19.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.20.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.20.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.20.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.20.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.21.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.21.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.21.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.21.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.22.self_attn.o_proj.weight' into RAM\n",
            "Loading 'layer 11 q' into RAM\n",
            "Loading 'layer 11 k' into RAM\n",
            "Loading 'layer 11 v' into RAM\n",
            "Loading 'layer 12 q' into RAM\n",
            "Loading 'layer 12 k' into RAM\n",
            "Loading 'layer 12 v' into RAM\n",
            "Loading 'layer 13 q' into RAM\n",
            "Loading 'layer 13 k' into RAM\n",
            "Loading 'layer 13 v' into RAM\n",
            "Loading 'layer 14 q' into RAM\n",
            "Loading 'layer 14 k' into RAM\n",
            "Loading 'layer 14 v' into RAM\n",
            "Loading 'layer 15 q' into RAM\n",
            "Loading 'layer 15 k' into RAM\n",
            "Loading 'layer 15 v' into RAM\n",
            "Loading 'layer 16 q' into RAM\n",
            "Loading 'layer 16 k' into RAM\n",
            "Loading 'layer 16 v' into RAM\n",
            "Loading 'layer 17 q' into RAM\n",
            "Loading 'layer 17 k' into RAM\n",
            "Loading 'layer 17 v' into RAM\n",
            "Loading 'layer 18 q' into RAM\n",
            "Loading 'layer 18 k' into RAM\n",
            "Loading 'layer 18 v' into RAM\n",
            "Loading 'layer 19 q' into RAM\n",
            "Loading 'layer 19 k' into RAM\n",
            "Loading 'layer 19 v' into RAM\n",
            "Loading 'layer 20 q' into RAM\n",
            "Loading 'layer 20 k' into RAM\n",
            "Loading 'layer 20 v' into RAM\n",
            "Loading 'layer 21 q' into RAM\n",
            "Loading 'layer 21 k' into RAM\n",
            "Loading 'layer 21 v' into RAM\n",
            "Loading 'layer 22 q' into RAM\n",
            "Loading 'layer 22 k' into RAM\n",
            "Loading 'layer 22 v' into RAM\n",
            "Processing checkpoints/mistralai/Mistral-7B-Instruct-v0.2/pytorch_model-00003-of-00003.bin\n",
            "Loading 'model.layers.22.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.22.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.22.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.22.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.23.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.23.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.23.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.23.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.24.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.24.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.24.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.24.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.25.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.25.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.25.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.25.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.26.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.26.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.26.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.26.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.27.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.27.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.27.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.27.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.28.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.28.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.28.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.28.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.29.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.29.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.29.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.29.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.30.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.30.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.30.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.layers.31.self_attn.o_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.gate_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.up_proj.weight' into RAM\n",
            "Loading 'model.layers.31.mlp.down_proj.weight' into RAM\n",
            "Loading 'model.layers.31.input_layernorm.weight' into RAM\n",
            "Loading 'model.layers.31.post_attention_layernorm.weight' into RAM\n",
            "Loading 'model.norm.weight' into RAM\n",
            "Loading 'lm_head.weight' into RAM\n",
            "Loading 'layer 23 q' into RAM\n",
            "Loading 'layer 23 k' into RAM\n",
            "Loading 'layer 23 v' into RAM\n",
            "Loading 'layer 24 q' into RAM\n",
            "Loading 'layer 24 k' into RAM\n",
            "Loading 'layer 24 v' into RAM\n",
            "Loading 'layer 25 q' into RAM\n",
            "Loading 'layer 25 k' into RAM\n",
            "Loading 'layer 25 v' into RAM\n",
            "Loading 'layer 26 q' into RAM\n",
            "Loading 'layer 26 k' into RAM\n",
            "Loading 'layer 26 v' into RAM\n",
            "Loading 'layer 27 q' into RAM\n",
            "Loading 'layer 27 k' into RAM\n",
            "Loading 'layer 27 v' into RAM\n",
            "Loading 'layer 28 q' into RAM\n",
            "Loading 'layer 28 k' into RAM\n",
            "Loading 'layer 28 v' into RAM\n",
            "Loading 'layer 29 q' into RAM\n",
            "Loading 'layer 29 k' into RAM\n",
            "Loading 'layer 29 v' into RAM\n",
            "Loading 'layer 30 q' into RAM\n",
            "Loading 'layer 30 k' into RAM\n",
            "Loading 'layer 30 v' into RAM\n",
            "Loading 'layer 31 q' into RAM\n",
            "Loading 'layer 31 k' into RAM\n",
            "Loading 'layer 31 v' into RAM\n",
            "Saving converted checkpoint\n",
            "usage: litgpt [-h] [-c CONFIG] [--print_config \b[=flags]]\n",
            "              {download,chat,finetune,pretrain,generate,convert,merge_lora} ...\n",
            "error: argument subcommand: invalid choice: 'convert_hf_checkpoint' (choose from 'download', 'chat', 'finetune', 'pretrain', 'generate', 'convert', 'merge_lora')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt generate base --precision bf16-true --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.2 --prompt \"Rohan is shorter than Seema. Krishna is taller than Pushpa but shorter than Anand. Dhiraj is taller than Krishna but shorter than Seema. Dhiraj is taller than Rohan. Question: Is Dhiraj the tallest ? Based on the the conetext and question is this entailment or not.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qtl4xJePf-40",
        "outputId": "debcfd30-9325-41b9-c98c-8a6be5e36f88"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/mistralai/Mistral-7B-Instruct-v0.2/lit_model.pth' with {'name': 'Mistral-7B-Instruct-v0.2', 'hf_config': {'name': 'Mistral-7B-Instruct-v0.2', 'org': 'mistralai'}, 'scale_embeddings': False, 'block_size': 32768, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'head_size': 128, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 128}\n",
            "Time to instantiate model: 0.36 seconds.\n",
            "Time to load the model weights: 80.88 seconds.\n",
            "Seed set to 1234\n",
            "<s>[INST] Rohan is shorter than Seema. Krishna is taller than Pushpa but shorter than Anand. Dhiraj is taller than Krishna but shorter than Seema. Dhiraj is taller than Rohan. Question: Is Dhiraj the tallest ? Based on the the conetext and question is this entailment or not. [/INST] Based on the information given, it is not entailed that Dhiraj is the tallest person mentioned.\n",
            "\n",
            "While Dhiraj is taller than Rohan and Krishna, there are still other individuals mentioned (Seema, P\n",
            "Time for inference 1: 3.46 sec total, 14.45 tokens/sec\n",
            "Memory used: 14.58 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data"
      ],
      "metadata": {
        "id": "4RdeeuN9pfmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tommymmcguire/logic-games-llms.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D_NKCG7OzJA",
        "outputId": "721a6b4f-5c7a-4aa3-8c9a-ebccc50a5b31"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'logic-games-llms' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"logic-games-llms\")\n",
        "!git pull\n",
        "os.chdir(\"..\")\n",
        "!pwd"
      ],
      "metadata": {
        "id": "bcFeR9UJgMpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f6bb55-e900-4837-a91c-9fd30f4dc01a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 1524, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/1524)\u001b[K\rremote: Counting objects:   1% (16/1524)\u001b[K\rremote: Counting objects:   2% (31/1524)\u001b[K\rremote: Counting objects:   3% (46/1524)\u001b[K\rremote: Counting objects:   4% (61/1524)\u001b[K\rremote: Counting objects:   5% (77/1524)\u001b[K\rremote: Counting objects:   6% (92/1524)\u001b[K\rremote: Counting objects:   7% (107/1524)\u001b[K\rremote: Counting objects:   8% (122/1524)\u001b[K\rremote: Counting objects:   9% (138/1524)\u001b[K\rremote: Counting objects:  10% (153/1524)\u001b[K\rremote: Counting objects:  11% (168/1524)\u001b[K\rremote: Counting objects:  12% (183/1524)\u001b[K\rremote: Counting objects:  13% (199/1524)\u001b[K\rremote: Counting objects:  14% (214/1524)\u001b[K\rremote: Counting objects:  15% (229/1524)\u001b[K\rremote: Counting objects:  16% (244/1524)\u001b[K\rremote: Counting objects:  17% (260/1524)\u001b[K\rremote: Counting objects:  18% (275/1524)\u001b[K\rremote: Counting objects:  19% (290/1524)\u001b[K\rremote: Counting objects:  20% (305/1524)\u001b[K\rremote: Counting objects:  21% (321/1524)\u001b[K\rremote: Counting objects:  22% (336/1524)\u001b[K\rremote: Counting objects:  23% (351/1524)\u001b[K\rremote: Counting objects:  24% (366/1524)\u001b[K\rremote: Counting objects:  25% (381/1524)\u001b[K\rremote: Counting objects:  26% (397/1524)\u001b[K\rremote: Counting objects:  27% (412/1524)\u001b[K\rremote: Counting objects:  28% (427/1524)\u001b[K\rremote: Counting objects:  29% (442/1524)\u001b[K\rremote: Counting objects:  30% (458/1524)\u001b[K\rremote: Counting objects:  31% (473/1524)\u001b[K\rremote: Counting objects:  32% (488/1524)\u001b[K\rremote: Counting objects:  33% (503/1524)\u001b[K\rremote: Counting objects:  34% (519/1524)\u001b[K\rremote: Counting objects:  35% (534/1524)\u001b[K\rremote: Counting objects:  36% (549/1524)\u001b[K\rremote: Counting objects:  37% (564/1524)\u001b[K\rremote: Counting objects:  38% (580/1524)\u001b[K\rremote: Counting objects:  39% (595/1524)\u001b[K\rremote: Counting objects:  40% (610/1524)\u001b[K\rremote: Counting objects:  41% (625/1524)\u001b[K\rremote: Counting objects:  42% (641/1524)\u001b[K\rremote: Counting objects:  43% (656/1524)\u001b[K\rremote: Counting objects:  44% (671/1524)\u001b[K\rremote: Counting objects:  45% (686/1524)\u001b[K\rremote: Counting objects:  46% (702/1524)\u001b[K\rremote: Counting objects:  47% (717/1524)\u001b[K\rremote: Counting objects:  48% (732/1524)\u001b[K\rremote: Counting objects:  49% (747/1524)\u001b[K\rremote: Counting objects:  50% (762/1524)\u001b[K\rremote: Counting objects:  51% (778/1524)\u001b[K\rremote: Counting objects:  52% (793/1524)\u001b[K\rremote: Counting objects:  53% (808/1524)\u001b[K\rremote: Counting objects:  54% (823/1524)\u001b[K\rremote: Counting objects:  55% (839/1524)\u001b[K\rremote: Counting objects:  56% (854/1524)\u001b[K\rremote: Counting objects:  57% (869/1524)\u001b[K\rremote: Counting objects:  58% (884/1524)\u001b[K\rremote: Counting objects:  59% (900/1524)\u001b[K\rremote: Counting objects:  60% (915/1524)\u001b[K\rremote: Counting objects:  61% (930/1524)\u001b[K\rremote: Counting objects:  62% (945/1524)\u001b[K\rremote: Counting objects:  63% (961/1524)\u001b[K\rremote: Counting objects:  64% (976/1524)\u001b[K\rremote: Counting objects:  65% (991/1524)\u001b[K\rremote: Counting objects:  66% (1006/1524)\u001b[K\rremote: Counting objects:  67% (1022/1524)\u001b[K\rremote: Counting objects:  68% (1037/1524)\u001b[K\rremote: Counting objects:  69% (1052/1524)\u001b[K\rremote: Counting objects:  70% (1067/1524)\u001b[K\rremote: Counting objects:  71% (1083/1524)\u001b[K\rremote: Counting objects:  72% (1098/1524)\u001b[K\rremote: Counting objects:  73% (1113/1524)\u001b[K\rremote: Counting objects:  74% (1128/1524)\u001b[K\rremote: Counting objects:  75% (1143/1524)\u001b[K\rremote: Counting objects:  76% (1159/1524)\u001b[K\rremote: Counting objects:  77% (1174/1524)\u001b[K\rremote: Counting objects:  78% (1189/1524)\u001b[K\rremote: Counting objects:  79% (1204/1524)\u001b[K\rremote: Counting objects:  80% (1220/1524)\u001b[K\rremote: Counting objects:  81% (1235/1524)\u001b[K\rremote: Counting objects:  82% (1250/1524)\u001b[K\rremote: Counting objects:  83% (1265/1524)\u001b[K\rremote: Counting objects:  84% (1281/1524)\u001b[K\rremote: Counting objects:  85% (1296/1524)\u001b[K\rremote: Counting objects:  86% (1311/1524)\u001b[K\rremote: Counting objects:  87% (1326/1524)\u001b[K\rremote: Counting objects:  88% (1342/1524)\u001b[K\rremote: Counting objects:  89% (1357/1524)\u001b[K\rremote: Counting objects:  90% (1372/1524)\u001b[K\rremote: Counting objects:  91% (1387/1524)\u001b[K\rremote: Counting objects:  92% (1403/1524)\u001b[K\rremote: Counting objects:  93% (1418/1524)\u001b[K\rremote: Counting objects:  94% (1433/1524)\u001b[K\rremote: Counting objects:  95% (1448/1524)\u001b[K\rremote: Counting objects:  96% (1464/1524)\u001b[K\rremote: Counting objects:  97% (1479/1524)\u001b[K\rremote: Counting objects:  98% (1494/1524)\u001b[K\rremote: Counting objects:  99% (1509/1524)\u001b[K\rremote: Counting objects: 100% (1524/1524)\u001b[K\rremote: Counting objects: 100% (1524/1524), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1411/1411), done.\u001b[K\n",
            "remote: Total 1520 (delta 96), reused 1513 (delta 92), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1520/1520), 5.42 MiB | 13.08 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), completed with 1 local object.\n",
            "From https://github.com/tommymmcguire/logic-games-llms\n",
            "   a3611a7..e1f7649  main       -> origin/main\n",
            " * [new branch]      nicks      -> origin/nicks\n",
            "Updating a3611a7..e1f7649\n",
            "Fast-forward\n",
            " .gitignore                |   160 \u001b[32m+\u001b[m\n",
            " data/sft-data/test.json   |  3752 \u001b[32m++++++++++\u001b[m\n",
            " data/sft-data/train.json  | 25002 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " data/sft-data/val.json    |  3752 \u001b[32m++++++++++\u001b[m\n",
            " notebooks/eval.ipynb      |    75 \u001b[32m+\u001b[m\n",
            " requirements.txt          |     2 \u001b[32m+\u001b[m\n",
            " scripts/build-sft-data.py |    22 \u001b[32m+\u001b[m\n",
            " 7 files changed, 32765 insertions(+)\n",
            " create mode 100644 .gitignore\n",
            " create mode 100644 data/sft-data/test.json\n",
            " create mode 100644 data/sft-data/train.json\n",
            " create mode 100644 data/sft-data/val.json\n",
            " create mode 100644 notebooks/eval.ipynb\n",
            " create mode 100644 scripts/build-sft-data.py\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune Model"
      ],
      "metadata": {
        "id": "KzV_1VpFpiQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt finetune lora \\\n",
        "  --checkpoint_dir \"checkpoints/mistralai/Mistral-7B-Instruct-v0.2/\" \\\n",
        "  --precision bf16-true \\\n",
        "  --quantize bnb.nf4 \\\n",
        "  --data JSON \\\n",
        "  --data.json_path \"logic-games-llms/data/sft-data/\" \\\n",
        "  --out_dir \"out4/lora_weights/mistral7B-finetuned/\" \\\n",
        "  --train.log_interval 10 \\\n",
        "  --train.save_interval 250 \\\n",
        "  --train.micro_batch_size 2 \\\n",
        "  --train.global_batch_size 32 \\\n",
        "  --train.epochs 3 \\\n",
        "  --train.lr_warmup_steps=100"
      ],
      "metadata": {
        "id": "AuwLFkdqpj-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352a69b0-de34-4824-b060-dc65dadd3a45"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'checkpoint_dir': PosixPath('checkpoints/mistralai/Mistral-7B-Instruct-v0.2'),\n",
            " 'data': JSON(json_path=PosixPath('logic-games-llms/data/sft-data'),\n",
            "              mask_prompt=False,\n",
            "              val_split_fraction=None,\n",
            "              prompt_style=<litgpt.prompts.Alpaca object at 0x7963a82cb190>,\n",
            "              ignore_index=-100,\n",
            "              seed=42,\n",
            "              num_workers=4),\n",
            " 'devices': 1,\n",
            " 'eval': EvalArgs(interval=100, max_new_tokens=100, max_iters=100),\n",
            " 'logger_name': 'csv',\n",
            " 'lora_alpha': 16,\n",
            " 'lora_dropout': 0.05,\n",
            " 'lora_head': False,\n",
            " 'lora_key': False,\n",
            " 'lora_mlp': False,\n",
            " 'lora_projection': False,\n",
            " 'lora_query': True,\n",
            " 'lora_r': 8,\n",
            " 'lora_value': True,\n",
            " 'out_dir': PosixPath('out4/lora_weights/mistral7B-finetuned'),\n",
            " 'precision': 'bf16-true',\n",
            " 'quantize': 'bnb.nf4',\n",
            " 'seed': 1337,\n",
            " 'train': TrainArgs(save_interval=250,\n",
            "                    log_interval=10,\n",
            "                    global_batch_size=32,\n",
            "                    micro_batch_size=2,\n",
            "                    lr_warmup_steps=100,\n",
            "                    epochs=3,\n",
            "                    max_tokens=None,\n",
            "                    max_steps=None,\n",
            "                    max_seq_length=None,\n",
            "                    tie_embeddings=None,\n",
            "                    learning_rate=0.0003,\n",
            "                    weight_decay=0.02,\n",
            "                    beta1=0.9,\n",
            "                    beta2=0.95,\n",
            "                    max_norm=None,\n",
            "                    min_lr=6e-05)}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 3,407,872\n",
            "Number of non trainable parameters: 7,241,732,096\n",
            "The longest sequence length in the train data is 518, the model's maximum sequence length is 518 and context length is 32768\n",
            "Validating ...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "Based on your preference for action films, I would recommend \"John Wick: Chapter 3 – Parabellum\". The movie continues the story of the former assassin, John Wick, as he is excommunicado from the international assassins guild, the High Table. To survive, John Wick must confront old friends, make new alliances, and fight his way through the criminal underworld. The movie offers non-stop action, thrilling stunts, and impressive\n",
            "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/throughput.py:587: 'Tesla V100-SXM2-16GB' does not support torch.bfloat16\n",
            "/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
            "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
            "Missing logger folder: out4/lora_weights/mistral7B-finetuned/logs/csv\n",
            "Epoch 1 | iter 10 step 0 | loss train: 2.686, val: n/a | iter time: 1210.97 ms\n",
            "Epoch 1 | iter 20 step 1 | loss train: 2.690, val: n/a | iter time: 2371.70 ms\n",
            "Epoch 1 | iter 30 step 1 | loss train: 2.614, val: n/a | iter time: 1729.52 ms\n",
            "Epoch 1 | iter 40 step 2 | loss train: 2.593, val: n/a | iter time: 1707.27 ms\n",
            "Epoch 1 | iter 50 step 3 | loss train: 2.534, val: n/a | iter time: 2006.50 ms\n",
            "Epoch 1 | iter 60 step 3 | loss train: 2.677, val: n/a | iter time: 1188.77 ms\n",
            "Epoch 1 | iter 70 step 4 | loss train: 2.615, val: n/a | iter time: 2037.94 ms\n",
            "Epoch 1 | iter 80 step 5 | loss train: 2.687, val: n/a | iter time: 1208.32 ms (step)\n",
            "Epoch 1 | iter 90 step 5 | loss train: 2.702, val: n/a | iter time: 1187.19 ms\n",
            "Epoch 1 | iter 100 step 6 | loss train: 2.689, val: n/a | iter time: 2637.53 ms\n",
            "Epoch 1 | iter 110 step 6 | loss train: 2.615, val: n/a | iter time: 1194.01 ms\n",
            "Epoch 1 | iter 120 step 7 | loss train: 2.564, val: n/a | iter time: 1866.10 ms\n",
            "Epoch 1 | iter 130 step 8 | loss train: 2.570, val: n/a | iter time: 2106.34 ms\n",
            "Epoch 1 | iter 140 step 8 | loss train: 2.522, val: n/a | iter time: 1190.21 ms\n",
            "Epoch 1 | iter 150 step 9 | loss train: 2.514, val: n/a | iter time: 1747.96 ms\n",
            "Epoch 1 | iter 160 step 10 | loss train: 2.485, val: n/a | iter time: 1869.07 ms (step)\n",
            "Epoch 1 | iter 170 step 10 | loss train: 2.433, val: n/a | iter time: 1217.48 ms\n",
            "Epoch 1 | iter 180 step 11 | loss train: 2.454, val: n/a | iter time: 1193.74 ms\n",
            "Epoch 1 | iter 190 step 11 | loss train: 2.430, val: n/a | iter time: 1187.85 ms\n",
            "Epoch 1 | iter 200 step 12 | loss train: 2.347, val: n/a | iter time: 1244.19 ms\n",
            "Epoch 1 | iter 210 step 13 | loss train: 2.278, val: n/a | iter time: 1194.03 ms\n",
            "Epoch 1 | iter 220 step 13 | loss train: 2.324, val: n/a | iter time: 1427.00 ms\n",
            "Epoch 1 | iter 230 step 14 | loss train: 2.237, val: n/a | iter time: 1190.73 ms\n",
            "Epoch 1 | iter 240 step 15 | loss train: 2.166, val: n/a | iter time: 1202.03 ms (step)\n",
            "Epoch 1 | iter 250 step 15 | loss train: 2.129, val: n/a | iter time: 1260.96 ms\n",
            "Epoch 1 | iter 260 step 16 | loss train: 2.113, val: n/a | iter time: 1187.76 ms\n",
            "Epoch 1 | iter 270 step 16 | loss train: 2.101, val: n/a | iter time: 1191.51 ms\n",
            "Epoch 1 | iter 280 step 17 | loss train: 2.057, val: n/a | iter time: 1613.18 ms\n",
            "Epoch 1 | iter 290 step 18 | loss train: 2.016, val: n/a | iter time: 2045.54 ms\n",
            "Epoch 1 | iter 300 step 18 | loss train: 1.954, val: n/a | iter time: 1592.99 ms\n",
            "Epoch 1 | iter 310 step 19 | loss train: 1.908, val: n/a | iter time: 1189.57 ms\n",
            "Epoch 1 | iter 320 step 20 | loss train: 1.929, val: n/a | iter time: 1608.96 ms (step)\n",
            "Epoch 1 | iter 330 step 20 | loss train: 1.862, val: n/a | iter time: 1113.99 ms\n",
            "Epoch 1 | iter 340 step 21 | loss train: 1.817, val: n/a | iter time: 1114.07 ms\n",
            "Epoch 1 | iter 350 step 21 | loss train: 1.736, val: n/a | iter time: 1235.26 ms\n",
            "Epoch 1 | iter 360 step 22 | loss train: 1.708, val: n/a | iter time: 1606.58 ms\n",
            "Epoch 1 | iter 370 step 23 | loss train: 1.659, val: n/a | iter time: 1750.45 ms\n",
            "Epoch 1 | iter 380 step 23 | loss train: 1.604, val: n/a | iter time: 1626.01 ms\n",
            "Epoch 1 | iter 390 step 24 | loss train: 1.600, val: n/a | iter time: 1750.77 ms\n",
            "Epoch 1 | iter 400 step 25 | loss train: 1.602, val: n/a | iter time: 1241.98 ms (step)\n",
            "Epoch 1 | iter 410 step 25 | loss train: 1.580, val: n/a | iter time: 1425.49 ms\n",
            "Epoch 1 | iter 420 step 26 | loss train: 1.487, val: n/a | iter time: 1208.14 ms\n",
            "Epoch 1 | iter 430 step 26 | loss train: 1.517, val: n/a | iter time: 1211.44 ms\n",
            "Epoch 1 | iter 440 step 27 | loss train: 1.441, val: n/a | iter time: 2043.18 ms\n",
            "Epoch 1 | iter 450 step 28 | loss train: 1.417, val: n/a | iter time: 3324.46 ms\n",
            "Epoch 1 | iter 460 step 28 | loss train: 1.400, val: n/a | iter time: 2008.02 ms\n",
            "Epoch 1 | iter 470 step 29 | loss train: 1.310, val: n/a | iter time: 1241.30 ms\n",
            "Epoch 1 | iter 480 step 30 | loss train: 1.286, val: n/a | iter time: 1429.53 ms (step)\n",
            "Epoch 1 | iter 490 step 30 | loss train: 1.270, val: n/a | iter time: 1236.19 ms\n",
            "Epoch 1 | iter 500 step 31 | loss train: 1.254, val: n/a | iter time: 1098.32 ms\n",
            "Epoch 1 | iter 510 step 31 | loss train: 1.169, val: n/a | iter time: 1869.74 ms\n",
            "Epoch 1 | iter 520 step 32 | loss train: 1.137, val: n/a | iter time: 1748.24 ms\n",
            "Epoch 1 | iter 530 step 33 | loss train: 1.100, val: n/a | iter time: 1192.61 ms\n",
            "Epoch 1 | iter 540 step 33 | loss train: 1.101, val: n/a | iter time: 1189.69 ms\n",
            "Epoch 1 | iter 550 step 34 | loss train: 1.069, val: n/a | iter time: 1188.28 ms\n",
            "Epoch 1 | iter 560 step 35 | loss train: 0.973, val: n/a | iter time: 2492.36 ms (step)\n",
            "Epoch 1 | iter 570 step 35 | loss train: 0.959, val: n/a | iter time: 1206.19 ms\n",
            "Epoch 1 | iter 580 step 36 | loss train: 0.969, val: n/a | iter time: 1423.53 ms\n",
            "Epoch 1 | iter 590 step 36 | loss train: 0.870, val: n/a | iter time: 1194.41 ms\n",
            "Epoch 1 | iter 600 step 37 | loss train: 0.890, val: n/a | iter time: 2042.97 ms\n",
            "Epoch 1 | iter 610 step 38 | loss train: 0.906, val: n/a | iter time: 1608.80 ms\n",
            "Epoch 1 | iter 620 step 38 | loss train: 0.843, val: n/a | iter time: 1186.83 ms\n",
            "Epoch 1 | iter 630 step 39 | loss train: 0.782, val: n/a | iter time: 1422.88 ms\n",
            "Epoch 1 | iter 640 step 40 | loss train: 0.780, val: n/a | iter time: 1875.97 ms (step)\n",
            "Epoch 1 | iter 650 step 40 | loss train: 0.807, val: n/a | iter time: 1091.08 ms\n",
            "Epoch 1 | iter 660 step 41 | loss train: 0.724, val: n/a | iter time: 1238.23 ms\n",
            "Epoch 1 | iter 670 step 41 | loss train: 0.715, val: n/a | iter time: 1190.04 ms\n",
            "Epoch 1 | iter 680 step 42 | loss train: 0.814, val: n/a | iter time: 1421.09 ms\n",
            "Epoch 1 | iter 690 step 43 | loss train: 0.702, val: n/a | iter time: 1195.84 ms\n",
            "Epoch 1 | iter 700 step 43 | loss train: 0.670, val: n/a | iter time: 1261.42 ms\n",
            "Epoch 1 | iter 710 step 44 | loss train: 0.667, val: n/a | iter time: 1235.13 ms\n",
            "Epoch 1 | iter 720 step 45 | loss train: 0.718, val: n/a | iter time: 1604.14 ms (step)\n",
            "Epoch 1 | iter 730 step 45 | loss train: 0.672, val: n/a | iter time: 1093.08 ms\n",
            "Epoch 1 | iter 740 step 46 | loss train: 0.695, val: n/a | iter time: 1190.89 ms\n",
            "Epoch 1 | iter 750 step 46 | loss train: 0.689, val: n/a | iter time: 1237.58 ms\n",
            "Epoch 1 | iter 760 step 47 | loss train: 0.629, val: n/a | iter time: 1192.92 ms\n",
            "Epoch 1 | iter 770 step 48 | loss train: 0.543, val: n/a | iter time: 1190.71 ms\n",
            "Epoch 1 | iter 780 step 48 | loss train: 0.554, val: n/a | iter time: 2129.90 ms\n",
            "Epoch 1 | iter 790 step 49 | loss train: 0.596, val: n/a | iter time: 1192.50 ms\n",
            "Epoch 1 | iter 800 step 50 | loss train: 0.629, val: n/a | iter time: 1755.67 ms (step)\n",
            "Epoch 1 | iter 810 step 50 | loss train: 0.597, val: n/a | iter time: 1423.34 ms\n",
            "Epoch 1 | iter 820 step 51 | loss train: 0.570, val: n/a | iter time: 1208.52 ms\n",
            "Epoch 1 | iter 830 step 51 | loss train: 0.587, val: n/a | iter time: 1235.00 ms\n",
            "Epoch 1 | iter 840 step 52 | loss train: 0.570, val: n/a | iter time: 1191.66 ms\n",
            "Epoch 1 | iter 850 step 53 | loss train: 0.573, val: n/a | iter time: 1200.16 ms\n",
            "Epoch 1 | iter 860 step 53 | loss train: 0.595, val: n/a | iter time: 1191.46 ms\n",
            "Epoch 1 | iter 870 step 54 | loss train: 0.573, val: n/a | iter time: 1191.89 ms\n",
            "Epoch 1 | iter 880 step 55 | loss train: 0.592, val: n/a | iter time: 1192.72 ms (step)\n",
            "Epoch 1 | iter 890 step 55 | loss train: 0.616, val: n/a | iter time: 1208.64 ms\n",
            "Epoch 1 | iter 900 step 56 | loss train: 0.568, val: n/a | iter time: 2093.82 ms\n",
            "Epoch 1 | iter 910 step 56 | loss train: 0.568, val: n/a | iter time: 1194.78 ms\n",
            "Epoch 1 | iter 920 step 57 | loss train: 0.448, val: n/a | iter time: 1236.56 ms\n",
            "Epoch 1 | iter 930 step 58 | loss train: 0.493, val: n/a | iter time: 1922.26 ms\n",
            "Epoch 1 | iter 940 step 58 | loss train: 0.513, val: n/a | iter time: 1880.57 ms\n",
            "Epoch 1 | iter 950 step 59 | loss train: 0.557, val: n/a | iter time: 1085.95 ms\n",
            "Epoch 1 | iter 960 step 60 | loss train: 0.531, val: n/a | iter time: 1213.30 ms (step)\n",
            "Epoch 1 | iter 970 step 60 | loss train: 0.505, val: n/a | iter time: 1237.66 ms\n",
            "Epoch 1 | iter 980 step 61 | loss train: 0.483, val: n/a | iter time: 2010.27 ms\n",
            "Epoch 1 | iter 990 step 61 | loss train: 0.489, val: n/a | iter time: 1091.55 ms\n",
            "Epoch 1 | iter 1000 step 62 | loss train: 0.501, val: n/a | iter time: 1188.73 ms\n",
            "Epoch 1 | iter 1010 step 63 | loss train: 0.554, val: n/a | iter time: 1213.63 ms\n",
            "Epoch 1 | iter 1020 step 63 | loss train: 0.501, val: n/a | iter time: 1258.24 ms\n",
            "Epoch 1 | iter 1030 step 64 | loss train: 0.480, val: n/a | iter time: 2014.40 ms\n",
            "Epoch 1 | iter 1040 step 65 | loss train: 0.456, val: n/a | iter time: 1195.98 ms (step)\n",
            "Epoch 1 | iter 1050 step 65 | loss train: 0.503, val: n/a | iter time: 1187.96 ms\n",
            "Epoch 1 | iter 1060 step 66 | loss train: 0.468, val: n/a | iter time: 1192.95 ms\n",
            "Epoch 1 | iter 1070 step 66 | loss train: 0.451, val: n/a | iter time: 1438.82 ms\n",
            "Epoch 1 | iter 1080 step 67 | loss train: 0.439, val: n/a | iter time: 1192.13 ms\n",
            "Epoch 1 | iter 1090 step 68 | loss train: 0.400, val: n/a | iter time: 1600.12 ms\n",
            "Epoch 1 | iter 1100 step 68 | loss train: 0.424, val: n/a | iter time: 2052.42 ms\n",
            "Epoch 1 | iter 1110 step 69 | loss train: 0.425, val: n/a | iter time: 1187.02 ms\n",
            "Epoch 1 | iter 1120 step 70 | loss train: 0.456, val: n/a | iter time: 1096.90 ms (step)\n",
            "Epoch 1 | iter 1130 step 70 | loss train: 0.405, val: n/a | iter time: 1192.32 ms\n",
            "Epoch 1 | iter 1140 step 71 | loss train: 0.404, val: n/a | iter time: 1236.81 ms\n",
            "Epoch 1 | iter 1150 step 71 | loss train: 0.397, val: n/a | iter time: 1422.59 ms\n",
            "Epoch 1 | iter 1160 step 72 | loss train: 0.441, val: n/a | iter time: 2024.14 ms\n",
            "Epoch 1 | iter 1170 step 73 | loss train: 0.428, val: n/a | iter time: 1194.57 ms\n",
            "Epoch 1 | iter 1180 step 73 | loss train: 0.359, val: n/a | iter time: 1237.30 ms\n",
            "Epoch 1 | iter 1190 step 74 | loss train: 0.349, val: n/a | iter time: 1238.66 ms\n",
            "Epoch 1 | iter 1200 step 75 | loss train: 0.403, val: n/a | iter time: 2405.50 ms (step)\n",
            "Epoch 1 | iter 1210 step 75 | loss train: 0.429, val: n/a | iter time: 1237.65 ms\n",
            "Epoch 1 | iter 1220 step 76 | loss train: 0.381, val: n/a | iter time: 1757.22 ms\n",
            "Epoch 1 | iter 1230 step 76 | loss train: 0.376, val: n/a | iter time: 1190.58 ms\n",
            "Epoch 1 | iter 1240 step 77 | loss train: 0.432, val: n/a | iter time: 1242.37 ms\n",
            "Epoch 1 | iter 1250 step 78 | loss train: 0.418, val: n/a | iter time: 1208.15 ms\n",
            "Epoch 1 | iter 1260 step 78 | loss train: 0.377, val: n/a | iter time: 1236.70 ms\n",
            "Epoch 1 | iter 1270 step 79 | loss train: 0.396, val: n/a | iter time: 1190.81 ms\n",
            "Epoch 1 | iter 1280 step 80 | loss train: 0.367, val: n/a | iter time: 1244.59 ms (step)\n",
            "Epoch 1 | iter 1290 step 80 | loss train: 0.340, val: n/a | iter time: 1192.42 ms\n",
            "Epoch 1 | iter 1300 step 81 | loss train: 0.416, val: n/a | iter time: 1650.98 ms\n",
            "Epoch 1 | iter 1310 step 81 | loss train: 0.452, val: n/a | iter time: 1234.90 ms\n",
            "Epoch 1 | iter 1320 step 82 | loss train: 0.377, val: n/a | iter time: 1235.62 ms\n",
            "Epoch 1 | iter 1330 step 83 | loss train: 0.306, val: n/a | iter time: 1235.61 ms\n",
            "Epoch 1 | iter 1340 step 83 | loss train: 0.334, val: n/a | iter time: 1242.62 ms\n",
            "Epoch 1 | iter 1350 step 84 | loss train: 0.381, val: n/a | iter time: 1188.42 ms\n",
            "Epoch 1 | iter 1360 step 85 | loss train: 0.394, val: n/a | iter time: 1616.17 ms (step)\n",
            "Epoch 1 | iter 1370 step 85 | loss train: 0.356, val: n/a | iter time: 1190.70 ms\n",
            "Epoch 1 | iter 1380 step 86 | loss train: 0.328, val: n/a | iter time: 2046.17 ms\n",
            "Epoch 1 | iter 1390 step 86 | loss train: 0.297, val: n/a | iter time: 1193.32 ms\n",
            "Epoch 1 | iter 1400 step 87 | loss train: 0.409, val: n/a | iter time: 1091.63 ms\n",
            "Epoch 1 | iter 1410 step 88 | loss train: 0.339, val: n/a | iter time: 1235.97 ms\n",
            "Epoch 1 | iter 1420 step 88 | loss train: 0.336, val: n/a | iter time: 1188.46 ms\n",
            "Epoch 1 | iter 1430 step 89 | loss train: 0.385, val: n/a | iter time: 2151.45 ms\n",
            "Epoch 1 | iter 1440 step 90 | loss train: 0.393, val: n/a | iter time: 1192.70 ms (step)\n",
            "Epoch 1 | iter 1450 step 90 | loss train: 0.337, val: n/a | iter time: 1190.78 ms\n",
            "Epoch 1 | iter 1460 step 91 | loss train: 0.336, val: n/a | iter time: 1238.86 ms\n",
            "Epoch 1 | iter 1470 step 91 | loss train: 0.369, val: n/a | iter time: 1718.71 ms\n",
            "Epoch 1 | iter 1480 step 92 | loss train: 0.411, val: n/a | iter time: 2096.12 ms\n",
            "Epoch 1 | iter 1490 step 93 | loss train: 0.382, val: n/a | iter time: 1729.12 ms\n",
            "Epoch 1 | iter 1500 step 93 | loss train: 0.360, val: n/a | iter time: 1192.88 ms\n",
            "Epoch 1 | iter 1510 step 94 | loss train: 0.307, val: n/a | iter time: 1092.07 ms\n",
            "Epoch 1 | iter 1520 step 95 | loss train: 0.305, val: n/a | iter time: 1424.46 ms (step)\n",
            "Epoch 1 | iter 1530 step 95 | loss train: 0.292, val: n/a | iter time: 2006.89 ms\n",
            "Epoch 1 | iter 1540 step 96 | loss train: 0.323, val: n/a | iter time: 1433.12 ms\n",
            "Epoch 1 | iter 1550 step 96 | loss train: 0.353, val: n/a | iter time: 2660.10 ms\n",
            "Epoch 1 | iter 1560 step 97 | loss train: 0.379, val: n/a | iter time: 1754.78 ms\n",
            "Epoch 1 | iter 1570 step 98 | loss train: 0.299, val: n/a | iter time: 1192.98 ms\n",
            "Epoch 1 | iter 1580 step 98 | loss train: 0.377, val: n/a | iter time: 1423.60 ms\n",
            "Epoch 1 | iter 1590 step 99 | loss train: 0.339, val: n/a | iter time: 1191.13 ms\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch 1 | iter 1600 step 100 | loss train: 0.301, val: n/a | iter time: 1245.92 ms (step)\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "Based on your preference for a mystery movie, I would recommend watching \"Murder on the Orient Express\" directed by Sidney Lumet. This movie is a classic whodunit story involving a group of passengers stranded on a train during a snowstorm. The detective Hercule Poirot must find the murderer before they kill again. I recommend this movie because it is a well-crafted mystery story and a good example of a closed-circle mystery.\n",
            "iter 1600: val loss 0.3241, val time: 82610.28 ms\n",
            "Epoch 1 | iter 1610 step 100 | loss train: 0.274, val: 0.324 | iter time: 1190.72 ms\n",
            "Epoch 1 | iter 1620 step 101 | loss train: 0.290, val: 0.324 | iter time: 1192.21 ms\n",
            "Epoch 1 | iter 1630 step 101 | loss train: 0.310, val: 0.324 | iter time: 2127.30 ms\n",
            "Epoch 1 | iter 1640 step 102 | loss train: 0.316, val: 0.324 | iter time: 1252.74 ms\n",
            "Epoch 1 | iter 1650 step 103 | loss train: 0.267, val: 0.324 | iter time: 1440.86 ms\n",
            "Epoch 1 | iter 1660 step 103 | loss train: 0.277, val: 0.324 | iter time: 1420.01 ms\n",
            "Epoch 1 | iter 1670 step 104 | loss train: 0.294, val: 0.324 | iter time: 2489.04 ms\n",
            "Epoch 1 | iter 1680 step 105 | loss train: 0.272, val: 0.324 | iter time: 1424.95 ms (step)\n",
            "Epoch 1 | iter 1690 step 105 | loss train: 0.242, val: 0.324 | iter time: 1251.11 ms\n",
            "Epoch 1 | iter 1700 step 106 | loss train: 0.359, val: 0.324 | iter time: 3432.85 ms\n",
            "Epoch 1 | iter 1710 step 106 | loss train: 0.370, val: 0.324 | iter time: 1926.72 ms\n",
            "Epoch 1 | iter 1720 step 107 | loss train: 0.313, val: 0.324 | iter time: 2045.48 ms\n",
            "Epoch 1 | iter 1730 step 108 | loss train: 0.277, val: 0.324 | iter time: 1189.95 ms\n",
            "Epoch 1 | iter 1740 step 108 | loss train: 0.274, val: 0.324 | iter time: 1236.85 ms\n",
            "Epoch 1 | iter 1750 step 109 | loss train: 0.238, val: 0.324 | iter time: 1256.57 ms\n",
            "Epoch 1 | iter 1760 step 110 | loss train: 0.223, val: 0.324 | iter time: 1243.56 ms (step)\n",
            "Epoch 1 | iter 1770 step 110 | loss train: 0.306, val: 0.324 | iter time: 1238.25 ms\n",
            "Epoch 1 | iter 1780 step 111 | loss train: 0.285, val: 0.324 | iter time: 1191.64 ms\n",
            "Epoch 1 | iter 1790 step 111 | loss train: 0.230, val: 0.324 | iter time: 1190.28 ms\n",
            "Epoch 1 | iter 1800 step 112 | loss train: 0.261, val: 0.324 | iter time: 1257.62 ms\n",
            "Epoch 1 | iter 1810 step 113 | loss train: 0.266, val: 0.324 | iter time: 1191.04 ms\n",
            "Epoch 1 | iter 1820 step 113 | loss train: 0.284, val: 0.324 | iter time: 1091.06 ms\n",
            "Epoch 1 | iter 1830 step 114 | loss train: 0.233, val: 0.324 | iter time: 1191.58 ms\n",
            "Epoch 1 | iter 1840 step 115 | loss train: 0.254, val: 0.324 | iter time: 1193.52 ms (step)\n",
            "Epoch 1 | iter 1850 step 115 | loss train: 0.259, val: 0.324 | iter time: 1420.66 ms\n",
            "Epoch 1 | iter 1860 step 116 | loss train: 0.245, val: 0.324 | iter time: 1191.12 ms\n",
            "Epoch 1 | iter 1870 step 116 | loss train: 0.242, val: 0.324 | iter time: 1191.13 ms\n",
            "Epoch 1 | iter 1880 step 117 | loss train: 0.257, val: 0.324 | iter time: 1191.75 ms\n",
            "Epoch 1 | iter 1890 step 118 | loss train: 0.235, val: 0.324 | iter time: 2016.33 ms\n",
            "Epoch 1 | iter 1900 step 118 | loss train: 0.290, val: 0.324 | iter time: 3438.33 ms\n",
            "Epoch 1 | iter 1910 step 119 | loss train: 0.318, val: 0.324 | iter time: 1250.75 ms\n",
            "Epoch 1 | iter 1920 step 120 | loss train: 0.296, val: 0.324 | iter time: 1095.50 ms (step)\n",
            "Epoch 1 | iter 1930 step 120 | loss train: 0.370, val: 0.324 | iter time: 2014.33 ms\n",
            "Epoch 1 | iter 1940 step 121 | loss train: 0.325, val: 0.324 | iter time: 1093.06 ms\n",
            "Epoch 1 | iter 1950 step 121 | loss train: 0.246, val: 0.324 | iter time: 2369.52 ms\n",
            "Epoch 1 | iter 1960 step 122 | loss train: 0.199, val: 0.324 | iter time: 1188.59 ms\n",
            "Epoch 1 | iter 1970 step 123 | loss train: 0.233, val: 0.324 | iter time: 1187.50 ms\n",
            "Epoch 1 | iter 1980 step 123 | loss train: 0.244, val: 0.324 | iter time: 1449.98 ms\n",
            "Epoch 1 | iter 1990 step 124 | loss train: 0.229, val: 0.324 | iter time: 2488.59 ms\n",
            "Epoch 1 | iter 2000 step 125 | loss train: 0.217, val: 0.324 | iter time: 1197.19 ms (step)\n",
            "Epoch 1 | iter 2010 step 125 | loss train: 0.238, val: 0.324 | iter time: 1092.99 ms\n",
            "Epoch 1 | iter 2020 step 126 | loss train: 0.224, val: 0.324 | iter time: 2043.84 ms\n",
            "Epoch 1 | iter 2030 step 126 | loss train: 0.213, val: 0.324 | iter time: 1611.24 ms\n",
            "Epoch 1 | iter 2040 step 127 | loss train: 0.228, val: 0.324 | iter time: 1206.61 ms\n",
            "Epoch 1 | iter 2050 step 128 | loss train: 0.227, val: 0.324 | iter time: 1423.26 ms\n",
            "Epoch 1 | iter 2060 step 128 | loss train: 0.216, val: 0.324 | iter time: 1190.71 ms\n",
            "Epoch 1 | iter 2070 step 129 | loss train: 0.217, val: 0.324 | iter time: 1235.42 ms\n",
            "Epoch 1 | iter 2080 step 130 | loss train: 0.288, val: 0.324 | iter time: 1743.01 ms (step)\n",
            "Epoch 1 | iter 2090 step 130 | loss train: 0.248, val: 0.324 | iter time: 1236.11 ms\n",
            "Epoch 1 | iter 2100 step 131 | loss train: 0.198, val: 0.324 | iter time: 1881.45 ms\n",
            "Epoch 1 | iter 2110 step 131 | loss train: 0.229, val: 0.324 | iter time: 1192.17 ms\n",
            "Epoch 1 | iter 2120 step 132 | loss train: 0.221, val: 0.324 | iter time: 1191.72 ms\n",
            "Epoch 1 | iter 2130 step 133 | loss train: 0.204, val: 0.324 | iter time: 1190.18 ms\n",
            "Epoch 1 | iter 2140 step 133 | loss train: 0.209, val: 0.324 | iter time: 1190.56 ms\n",
            "Epoch 1 | iter 2150 step 134 | loss train: 0.232, val: 0.324 | iter time: 1893.76 ms\n",
            "Epoch 1 | iter 2160 step 135 | loss train: 0.206, val: 0.324 | iter time: 1192.12 ms (step)\n",
            "Epoch 1 | iter 2170 step 135 | loss train: 0.203, val: 0.324 | iter time: 1719.24 ms\n",
            "Epoch 1 | iter 2180 step 136 | loss train: 0.214, val: 0.324 | iter time: 1234.08 ms\n",
            "Epoch 1 | iter 2190 step 136 | loss train: 0.212, val: 0.324 | iter time: 1212.59 ms\n",
            "Epoch 1 | iter 2200 step 137 | loss train: 0.254, val: 0.324 | iter time: 1190.65 ms\n",
            "Epoch 1 | iter 2210 step 138 | loss train: 0.241, val: 0.324 | iter time: 2015.20 ms\n",
            "Epoch 1 | iter 2220 step 138 | loss train: 0.224, val: 0.324 | iter time: 1241.02 ms\n",
            "Epoch 1 | iter 2230 step 139 | loss train: 0.235, val: 0.324 | iter time: 2115.86 ms\n",
            "Epoch 1 | iter 2240 step 140 | loss train: 0.248, val: 0.324 | iter time: 1197.02 ms (step)\n",
            "Epoch 1 | iter 2250 step 140 | loss train: 0.206, val: 0.324 | iter time: 2043.57 ms\n",
            "Epoch 1 | iter 2260 step 141 | loss train: 0.201, val: 0.324 | iter time: 1192.65 ms\n",
            "Epoch 1 | iter 2270 step 141 | loss train: 0.191, val: 0.324 | iter time: 1190.93 ms\n",
            "Epoch 1 | iter 2280 step 142 | loss train: 0.209, val: 0.324 | iter time: 2117.60 ms\n",
            "Epoch 1 | iter 2290 step 143 | loss train: 0.211, val: 0.324 | iter time: 1238.36 ms\n",
            "Epoch 1 | iter 2300 step 143 | loss train: 0.226, val: 0.324 | iter time: 1192.46 ms\n",
            "Epoch 1 | iter 2310 step 144 | loss train: 0.191, val: 0.324 | iter time: 1091.80 ms\n",
            "Epoch 1 | iter 2320 step 145 | loss train: 0.175, val: 0.324 | iter time: 1616.98 ms (step)\n",
            "Epoch 1 | iter 2330 step 145 | loss train: 0.212, val: 0.324 | iter time: 1236.98 ms\n",
            "Epoch 1 | iter 2340 step 146 | loss train: 0.224, val: 0.324 | iter time: 1236.71 ms\n",
            "Epoch 1 | iter 2350 step 146 | loss train: 0.220, val: 0.324 | iter time: 1262.13 ms\n",
            "Epoch 1 | iter 2360 step 147 | loss train: 0.206, val: 0.324 | iter time: 1187.92 ms\n",
            "Epoch 1 | iter 2370 step 148 | loss train: 0.186, val: 0.324 | iter time: 1193.79 ms\n",
            "Epoch 1 | iter 2380 step 148 | loss train: 0.220, val: 0.324 | iter time: 1911.41 ms\n",
            "Epoch 1 | iter 2390 step 149 | loss train: 0.233, val: 0.324 | iter time: 1736.23 ms\n",
            "Epoch 1 | iter 2400 step 150 | loss train: 0.215, val: 0.324 | iter time: 1094.16 ms (step)\n",
            "Epoch 1 | iter 2410 step 150 | loss train: 0.207, val: 0.324 | iter time: 1754.72 ms\n",
            "Epoch 1 | iter 2420 step 151 | loss train: 0.210, val: 0.324 | iter time: 1194.09 ms\n",
            "Epoch 1 | iter 2430 step 151 | loss train: 0.202, val: 0.324 | iter time: 1235.11 ms\n",
            "Epoch 1 | iter 2440 step 152 | loss train: 0.188, val: 0.324 | iter time: 1190.88 ms\n",
            "Epoch 1 | iter 2450 step 153 | loss train: 0.215, val: 0.324 | iter time: 1209.48 ms\n",
            "Epoch 1 | iter 2460 step 153 | loss train: 0.202, val: 0.324 | iter time: 2043.24 ms\n",
            "Epoch 1 | iter 2470 step 154 | loss train: 0.228, val: 0.324 | iter time: 1238.18 ms\n",
            "Epoch 1 | iter 2480 step 155 | loss train: 0.239, val: 0.324 | iter time: 1425.38 ms (step)\n",
            "Epoch 1 | iter 2490 step 155 | loss train: 0.243, val: 0.324 | iter time: 3325.54 ms\n",
            "Epoch 1 | iter 2500 step 156 | loss train: 0.262, val: 0.324 | iter time: 1931.86 ms\n",
            "Epoch 2 | iter 2510 step 156 | loss train: 0.227, val: 0.324 | iter time: 1089.01 ms\n",
            "Epoch 2 | iter 2520 step 157 | loss train: 0.192, val: 0.324 | iter time: 2014.82 ms\n",
            "Epoch 2 | iter 2530 step 158 | loss train: 0.201, val: 0.324 | iter time: 1918.09 ms\n",
            "Epoch 2 | iter 2540 step 158 | loss train: 0.207, val: 0.324 | iter time: 1127.76 ms\n",
            "Epoch 2 | iter 2550 step 159 | loss train: 0.193, val: 0.324 | iter time: 1178.63 ms\n",
            "Epoch 2 | iter 2560 step 160 | loss train: 0.178, val: 0.324 | iter time: 1198.30 ms (step)\n",
            "Epoch 2 | iter 2570 step 160 | loss train: 0.177, val: 0.324 | iter time: 1755.79 ms\n",
            "Epoch 2 | iter 2580 step 161 | loss train: 0.186, val: 0.324 | iter time: 1188.52 ms\n",
            "Epoch 2 | iter 2590 step 161 | loss train: 0.158, val: 0.324 | iter time: 1194.94 ms\n",
            "Epoch 2 | iter 2600 step 162 | loss train: 0.186, val: 0.324 | iter time: 1186.75 ms\n",
            "Epoch 2 | iter 2610 step 163 | loss train: 0.212, val: 0.324 | iter time: 1238.22 ms\n",
            "Epoch 2 | iter 2620 step 163 | loss train: 0.213, val: 0.324 | iter time: 2091.05 ms\n",
            "Epoch 2 | iter 2630 step 164 | loss train: 0.193, val: 0.324 | iter time: 1090.79 ms\n",
            "Epoch 2 | iter 2640 step 165 | loss train: 0.180, val: 0.324 | iter time: 1193.23 ms (step)\n",
            "Epoch 2 | iter 2650 step 165 | loss train: 0.164, val: 0.324 | iter time: 1757.57 ms\n",
            "Epoch 2 | iter 2660 step 166 | loss train: 0.173, val: 0.324 | iter time: 1235.70 ms\n",
            "Epoch 2 | iter 2670 step 166 | loss train: 0.180, val: 0.324 | iter time: 1763.83 ms\n",
            "Epoch 2 | iter 2680 step 167 | loss train: 0.195, val: 0.324 | iter time: 1207.09 ms\n",
            "Epoch 2 | iter 2690 step 168 | loss train: 0.178, val: 0.324 | iter time: 1199.51 ms\n",
            "Epoch 2 | iter 2700 step 168 | loss train: 0.188, val: 0.324 | iter time: 1193.77 ms\n",
            "Epoch 2 | iter 2710 step 169 | loss train: 0.169, val: 0.324 | iter time: 1235.90 ms\n",
            "Epoch 2 | iter 2720 step 170 | loss train: 0.174, val: 0.324 | iter time: 1919.16 ms (step)\n",
            "Epoch 2 | iter 2730 step 170 | loss train: 0.204, val: 0.324 | iter time: 1623.57 ms\n",
            "Epoch 2 | iter 2740 step 171 | loss train: 0.181, val: 0.324 | iter time: 1221.43 ms\n",
            "Epoch 2 | iter 2750 step 171 | loss train: 0.184, val: 0.324 | iter time: 1194.31 ms\n",
            "Epoch 2 | iter 2760 step 172 | loss train: 0.161, val: 0.324 | iter time: 2758.13 ms\n",
            "Epoch 2 | iter 2770 step 173 | loss train: 0.177, val: 0.324 | iter time: 1197.84 ms\n",
            "Epoch 2 | iter 2780 step 173 | loss train: 0.172, val: 0.324 | iter time: 1236.48 ms\n",
            "Epoch 2 | iter 2790 step 174 | loss train: 0.193, val: 0.324 | iter time: 1879.81 ms\n",
            "Epoch 2 | iter 2800 step 175 | loss train: 0.184, val: 0.324 | iter time: 1430.69 ms (step)\n",
            "Epoch 2 | iter 2810 step 175 | loss train: 0.142, val: 0.324 | iter time: 1899.53 ms\n",
            "Epoch 2 | iter 2820 step 176 | loss train: 0.175, val: 0.324 | iter time: 1191.69 ms\n",
            "Epoch 2 | iter 2830 step 176 | loss train: 0.176, val: 0.324 | iter time: 1177.43 ms\n",
            "Epoch 2 | iter 2840 step 177 | loss train: 0.175, val: 0.324 | iter time: 1879.94 ms\n",
            "Epoch 2 | iter 2850 step 178 | loss train: 0.176, val: 0.324 | iter time: 2644.93 ms\n",
            "Epoch 2 | iter 2860 step 178 | loss train: 0.185, val: 0.324 | iter time: 1195.35 ms\n",
            "Epoch 2 | iter 2870 step 179 | loss train: 0.181, val: 0.324 | iter time: 2133.53 ms\n",
            "Epoch 2 | iter 2880 step 180 | loss train: 0.176, val: 0.324 | iter time: 2150.56 ms (step)\n",
            "Epoch 2 | iter 2890 step 180 | loss train: 0.154, val: 0.324 | iter time: 2011.57 ms\n",
            "Epoch 2 | iter 2900 step 181 | loss train: 0.176, val: 0.324 | iter time: 1187.35 ms\n",
            "Epoch 2 | iter 2910 step 181 | loss train: 0.170, val: 0.324 | iter time: 1258.68 ms\n",
            "Epoch 2 | iter 2920 step 182 | loss train: 0.174, val: 0.324 | iter time: 1190.16 ms\n",
            "Epoch 2 | iter 2930 step 183 | loss train: 0.166, val: 0.324 | iter time: 1608.13 ms\n",
            "Epoch 2 | iter 2940 step 183 | loss train: 0.175, val: 0.324 | iter time: 1188.23 ms\n",
            "Epoch 2 | iter 2950 step 184 | loss train: 0.134, val: 0.324 | iter time: 2493.08 ms\n",
            "Epoch 2 | iter 2960 step 185 | loss train: 0.140, val: 0.324 | iter time: 1238.63 ms (step)\n",
            "Epoch 2 | iter 2970 step 185 | loss train: 0.171, val: 0.324 | iter time: 1250.45 ms\n",
            "Epoch 2 | iter 2980 step 186 | loss train: 0.179, val: 0.324 | iter time: 2393.20 ms\n",
            "Epoch 2 | iter 2990 step 186 | loss train: 0.156, val: 0.324 | iter time: 2647.53 ms\n",
            "Epoch 2 | iter 3000 step 187 | loss train: 0.146, val: 0.324 | iter time: 1188.61 ms\n",
            "Epoch 2 | iter 3010 step 188 | loss train: 0.170, val: 0.324 | iter time: 2011.65 ms\n",
            "Epoch 2 | iter 3020 step 188 | loss train: 0.169, val: 0.324 | iter time: 1193.31 ms\n",
            "Epoch 2 | iter 3030 step 189 | loss train: 0.146, val: 0.324 | iter time: 1255.06 ms\n",
            "Epoch 2 | iter 3040 step 190 | loss train: 0.132, val: 0.324 | iter time: 1240.46 ms (step)\n",
            "Epoch 2 | iter 3050 step 190 | loss train: 0.130, val: 0.324 | iter time: 1239.40 ms\n",
            "Epoch 2 | iter 3060 step 191 | loss train: 0.151, val: 0.324 | iter time: 3431.56 ms\n",
            "Epoch 2 | iter 3070 step 191 | loss train: 0.176, val: 0.324 | iter time: 1190.31 ms\n",
            "Epoch 2 | iter 3080 step 192 | loss train: 0.160, val: 0.324 | iter time: 1190.27 ms\n",
            "Epoch 2 | iter 3090 step 193 | loss train: 0.157, val: 0.324 | iter time: 1709.79 ms\n",
            "Epoch 2 | iter 3100 step 193 | loss train: 0.146, val: 0.324 | iter time: 1208.13 ms\n",
            "Epoch 2 | iter 3110 step 194 | loss train: 0.133, val: 0.324 | iter time: 1210.04 ms\n",
            "Epoch 2 | iter 3120 step 195 | loss train: 0.127, val: 0.324 | iter time: 1239.77 ms (step)\n",
            "Epoch 2 | iter 3130 step 195 | loss train: 0.137, val: 0.324 | iter time: 1189.17 ms\n",
            "Epoch 2 | iter 3140 step 196 | loss train: 0.134, val: 0.324 | iter time: 1751.74 ms\n",
            "Epoch 2 | iter 3150 step 196 | loss train: 0.134, val: 0.324 | iter time: 1194.62 ms\n",
            "Epoch 2 | iter 3160 step 197 | loss train: 0.136, val: 0.324 | iter time: 1205.94 ms\n",
            "Epoch 2 | iter 3170 step 198 | loss train: 0.116, val: 0.324 | iter time: 1882.18 ms\n",
            "Epoch 2 | iter 3180 step 198 | loss train: 0.143, val: 0.324 | iter time: 1424.46 ms\n",
            "Epoch 2 | iter 3190 step 199 | loss train: 0.134, val: 0.324 | iter time: 1235.25 ms\n",
            "Epoch 2 | iter 3200 step 200 | loss train: 0.140, val: 0.324 | iter time: 1638.36 ms (step)\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "Based on your preference for a romantic movie, I would recommend \"The Notebook\" for you to watch during the weekend. This movie is a Below average romance that will leave you with a satisfying feeling.\n",
            "iter 3200: val loss 0.1520, val time: 76907.40 ms\n",
            "Epoch 2 | iter 3210 step 200 | loss train: 0.127, val: 0.152 | iter time: 1235.93 ms\n",
            "Epoch 2 | iter 3220 step 201 | loss train: 0.115, val: 0.152 | iter time: 1614.57 ms\n",
            "Epoch 2 | iter 3230 step 201 | loss train: 0.162, val: 0.152 | iter time: 1208.00 ms\n",
            "Epoch 2 | iter 3240 step 202 | loss train: 0.142, val: 0.152 | iter time: 1442.16 ms\n",
            "Epoch 2 | iter 3250 step 203 | loss train: 0.118, val: 0.152 | iter time: 1235.32 ms\n",
            "Epoch 2 | iter 3260 step 203 | loss train: 0.110, val: 0.152 | iter time: 1720.14 ms\n",
            "Epoch 2 | iter 3270 step 204 | loss train: 0.134, val: 0.152 | iter time: 1236.29 ms\n",
            "Epoch 2 | iter 3280 step 205 | loss train: 0.149, val: 0.152 | iter time: 1184.03 ms (step)\n",
            "Epoch 2 | iter 3290 step 205 | loss train: 0.159, val: 0.152 | iter time: 2045.77 ms\n",
            "Epoch 2 | iter 3300 step 206 | loss train: 0.139, val: 0.152 | iter time: 1190.60 ms\n",
            "Epoch 2 | iter 3310 step 206 | loss train: 0.143, val: 0.152 | iter time: 1609.92 ms\n",
            "Epoch 2 | iter 3320 step 207 | loss train: 0.145, val: 0.152 | iter time: 1254.97 ms\n",
            "Epoch 2 | iter 3330 step 208 | loss train: 0.136, val: 0.152 | iter time: 1190.34 ms\n",
            "Epoch 2 | iter 3340 step 208 | loss train: 0.141, val: 0.152 | iter time: 2096.90 ms\n",
            "Epoch 2 | iter 3350 step 209 | loss train: 0.153, val: 0.152 | iter time: 1191.04 ms\n",
            "Epoch 2 | iter 3360 step 210 | loss train: 0.141, val: 0.152 | iter time: 1632.72 ms (step)\n",
            "Epoch 2 | iter 3370 step 210 | loss train: 0.130, val: 0.152 | iter time: 1258.65 ms\n",
            "Epoch 2 | iter 3380 step 211 | loss train: 0.121, val: 0.152 | iter time: 1236.90 ms\n",
            "Epoch 2 | iter 3390 step 211 | loss train: 0.110, val: 0.152 | iter time: 1428.26 ms\n",
            "Epoch 2 | iter 3400 step 212 | loss train: 0.117, val: 0.152 | iter time: 1192.32 ms\n",
            "Epoch 2 | iter 3410 step 213 | loss train: 0.119, val: 0.152 | iter time: 1085.78 ms\n",
            "Epoch 2 | iter 3420 step 213 | loss train: 0.137, val: 0.152 | iter time: 1251.80 ms\n",
            "Epoch 2 | iter 3430 step 214 | loss train: 0.139, val: 0.152 | iter time: 1092.86 ms\n",
            "Epoch 2 | iter 3440 step 215 | loss train: 0.125, val: 0.152 | iter time: 1091.87 ms (step)\n",
            "Epoch 2 | iter 3450 step 215 | loss train: 0.140, val: 0.152 | iter time: 1428.18 ms\n",
            "Epoch 2 | iter 3460 step 216 | loss train: 0.144, val: 0.152 | iter time: 1190.44 ms\n",
            "Epoch 2 | iter 3470 step 216 | loss train: 0.135, val: 0.152 | iter time: 1186.88 ms\n",
            "Epoch 2 | iter 3480 step 217 | loss train: 0.109, val: 0.152 | iter time: 1236.13 ms\n",
            "Epoch 2 | iter 3490 step 218 | loss train: 0.124, val: 0.152 | iter time: 1245.48 ms\n",
            "Epoch 2 | iter 3500 step 218 | loss train: 0.141, val: 0.152 | iter time: 1931.99 ms\n",
            "Epoch 2 | iter 3510 step 219 | loss train: 0.157, val: 0.152 | iter time: 2012.82 ms\n",
            "Epoch 2 | iter 3520 step 220 | loss train: 0.148, val: 0.152 | iter time: 1922.47 ms (step)\n",
            "Epoch 2 | iter 3530 step 220 | loss train: 0.130, val: 0.152 | iter time: 1608.26 ms\n",
            "Epoch 2 | iter 3540 step 221 | loss train: 0.112, val: 0.152 | iter time: 1617.09 ms\n",
            "Epoch 2 | iter 3550 step 221 | loss train: 0.117, val: 0.152 | iter time: 2119.88 ms\n",
            "Epoch 2 | iter 3560 step 222 | loss train: 0.124, val: 0.152 | iter time: 1189.48 ms\n",
            "Epoch 2 | iter 3570 step 223 | loss train: 0.125, val: 0.152 | iter time: 1610.85 ms\n",
            "Epoch 2 | iter 3580 step 223 | loss train: 0.144, val: 0.152 | iter time: 1235.58 ms\n",
            "Epoch 2 | iter 3590 step 224 | loss train: 0.116, val: 0.152 | iter time: 1090.83 ms\n",
            "Epoch 2 | iter 3600 step 225 | loss train: 0.117, val: 0.152 | iter time: 1203.59 ms (step)\n",
            "Epoch 2 | iter 3610 step 225 | loss train: 0.133, val: 0.152 | iter time: 1235.58 ms\n",
            "Epoch 2 | iter 3620 step 226 | loss train: 0.142, val: 0.152 | iter time: 1191.29 ms\n",
            "Epoch 2 | iter 3630 step 226 | loss train: 0.128, val: 0.152 | iter time: 1190.44 ms\n",
            "Epoch 2 | iter 3640 step 227 | loss train: 0.137, val: 0.152 | iter time: 1235.58 ms\n",
            "Epoch 2 | iter 3650 step 228 | loss train: 0.153, val: 0.152 | iter time: 1204.00 ms\n",
            "Epoch 2 | iter 3660 step 228 | loss train: 0.116, val: 0.152 | iter time: 1249.20 ms\n",
            "Epoch 2 | iter 3670 step 229 | loss train: 0.118, val: 0.152 | iter time: 1609.63 ms\n",
            "Epoch 2 | iter 3680 step 230 | loss train: 0.130, val: 0.152 | iter time: 1241.81 ms (step)\n",
            "Epoch 2 | iter 3690 step 230 | loss train: 0.118, val: 0.152 | iter time: 2491.16 ms\n",
            "Epoch 2 | iter 3700 step 231 | loss train: 0.146, val: 0.152 | iter time: 1091.31 ms\n",
            "Epoch 2 | iter 3710 step 231 | loss train: 0.152, val: 0.152 | iter time: 1238.56 ms\n",
            "Epoch 2 | iter 3720 step 232 | loss train: 0.126, val: 0.152 | iter time: 1235.66 ms\n",
            "Epoch 2 | iter 3730 step 233 | loss train: 0.115, val: 0.152 | iter time: 1195.73 ms\n",
            "Epoch 2 | iter 3740 step 233 | loss train: 0.151, val: 0.152 | iter time: 1228.56 ms\n",
            "Epoch 2 | iter 3750 step 234 | loss train: 0.152, val: 0.152 | iter time: 1916.65 ms\n",
            "Epoch 2 | iter 3760 step 235 | loss train: 0.130, val: 0.152 | iter time: 2012.22 ms (step)\n",
            "Epoch 2 | iter 3770 step 235 | loss train: 0.152, val: 0.152 | iter time: 1880.73 ms\n",
            "Epoch 2 | iter 3780 step 236 | loss train: 0.140, val: 0.152 | iter time: 1192.09 ms\n",
            "Epoch 2 | iter 3790 step 236 | loss train: 0.150, val: 0.152 | iter time: 2049.08 ms\n",
            "Epoch 2 | iter 3800 step 237 | loss train: 0.135, val: 0.152 | iter time: 3431.97 ms\n",
            "Epoch 2 | iter 3810 step 238 | loss train: 0.123, val: 0.152 | iter time: 1430.20 ms\n",
            "Epoch 2 | iter 3820 step 238 | loss train: 0.133, val: 0.152 | iter time: 1270.53 ms\n",
            "Epoch 2 | iter 3830 step 239 | loss train: 0.119, val: 0.152 | iter time: 1194.75 ms\n",
            "Epoch 2 | iter 3840 step 240 | loss train: 0.136, val: 0.152 | iter time: 1884.81 ms (step)\n",
            "Epoch 2 | iter 3850 step 240 | loss train: 0.146, val: 0.152 | iter time: 2637.51 ms\n",
            "Epoch 2 | iter 3860 step 241 | loss train: 0.121, val: 0.152 | iter time: 1238.01 ms\n",
            "Epoch 2 | iter 3870 step 241 | loss train: 0.123, val: 0.152 | iter time: 1193.36 ms\n",
            "Epoch 2 | iter 3880 step 242 | loss train: 0.144, val: 0.152 | iter time: 1190.91 ms\n",
            "Epoch 2 | iter 3890 step 243 | loss train: 0.129, val: 0.152 | iter time: 2639.40 ms\n",
            "Epoch 2 | iter 3900 step 243 | loss train: 0.131, val: 0.152 | iter time: 1238.23 ms\n",
            "Epoch 2 | iter 3910 step 244 | loss train: 0.161, val: 0.152 | iter time: 2017.30 ms\n",
            "Epoch 2 | iter 3920 step 245 | loss train: 0.127, val: 0.152 | iter time: 1205.58 ms (step)\n",
            "Epoch 2 | iter 3930 step 245 | loss train: 0.134, val: 0.152 | iter time: 1193.62 ms\n",
            "Epoch 2 | iter 3940 step 246 | loss train: 0.110, val: 0.152 | iter time: 1192.46 ms\n",
            "Epoch 2 | iter 3950 step 246 | loss train: 0.134, val: 0.152 | iter time: 1240.19 ms\n",
            "Epoch 2 | iter 3960 step 247 | loss train: 0.114, val: 0.152 | iter time: 1253.40 ms\n",
            "Epoch 2 | iter 3970 step 248 | loss train: 0.125, val: 0.152 | iter time: 3465.67 ms\n",
            "Epoch 2 | iter 3980 step 248 | loss train: 0.117, val: 0.152 | iter time: 1091.16 ms\n",
            "Epoch 2 | iter 3990 step 249 | loss train: 0.141, val: 0.152 | iter time: 1876.50 ms\n",
            "Epoch 2 | iter 4000 step 250 | loss train: 0.118, val: 0.152 | iter time: 1200.89 ms (step)\n",
            "Saving LoRA weights to 'out4/lora_weights/mistral7B-finetuned/step-000250/lit_model.pth.lora'\n",
            "Epoch 2 | iter 4010 step 250 | loss train: 0.113, val: 0.152 | iter time: 1237.00 ms\n",
            "Epoch 2 | iter 4020 step 251 | loss train: 0.133, val: 0.152 | iter time: 1189.93 ms\n",
            "Epoch 2 | iter 4030 step 251 | loss train: 0.127, val: 0.152 | iter time: 1191.78 ms\n",
            "Epoch 2 | iter 4040 step 252 | loss train: 0.108, val: 0.152 | iter time: 1215.06 ms\n",
            "Epoch 2 | iter 4050 step 253 | loss train: 0.104, val: 0.152 | iter time: 1240.12 ms\n",
            "Epoch 2 | iter 4060 step 253 | loss train: 0.108, val: 0.152 | iter time: 1190.94 ms\n",
            "Epoch 2 | iter 4070 step 254 | loss train: 0.110, val: 0.152 | iter time: 2369.63 ms\n",
            "Epoch 2 | iter 4080 step 255 | loss train: 0.119, val: 0.152 | iter time: 1938.36 ms (step)\n",
            "Epoch 2 | iter 4090 step 255 | loss train: 0.139, val: 0.152 | iter time: 1425.26 ms\n",
            "Epoch 2 | iter 4100 step 256 | loss train: 0.137, val: 0.152 | iter time: 1191.56 ms\n",
            "Epoch 2 | iter 4110 step 256 | loss train: 0.122, val: 0.152 | iter time: 1193.07 ms\n",
            "Epoch 2 | iter 4120 step 257 | loss train: 0.109, val: 0.152 | iter time: 1432.90 ms\n",
            "Epoch 2 | iter 4130 step 258 | loss train: 0.133, val: 0.152 | iter time: 1239.61 ms\n",
            "Epoch 2 | iter 4140 step 258 | loss train: 0.158, val: 0.152 | iter time: 2043.32 ms\n",
            "Epoch 2 | iter 4150 step 259 | loss train: 0.138, val: 0.152 | iter time: 1189.41 ms\n",
            "Epoch 2 | iter 4160 step 260 | loss train: 0.112, val: 0.152 | iter time: 1241.83 ms (step)\n",
            "Epoch 2 | iter 4170 step 260 | loss train: 0.132, val: 0.152 | iter time: 1204.57 ms\n",
            "Epoch 2 | iter 4180 step 261 | loss train: 0.126, val: 0.152 | iter time: 1191.55 ms\n",
            "Epoch 2 | iter 4190 step 261 | loss train: 0.120, val: 0.152 | iter time: 1187.99 ms\n",
            "Epoch 2 | iter 4200 step 262 | loss train: 0.132, val: 0.152 | iter time: 1236.32 ms\n",
            "Epoch 2 | iter 4210 step 263 | loss train: 0.132, val: 0.152 | iter time: 1423.12 ms\n",
            "Epoch 2 | iter 4220 step 263 | loss train: 0.123, val: 0.152 | iter time: 2028.98 ms\n",
            "Epoch 2 | iter 4230 step 264 | loss train: 0.111, val: 0.152 | iter time: 1236.48 ms\n",
            "Epoch 2 | iter 4240 step 265 | loss train: 0.108, val: 0.152 | iter time: 1249.54 ms (step)\n",
            "Epoch 2 | iter 4250 step 265 | loss train: 0.112, val: 0.152 | iter time: 1090.14 ms\n",
            "Epoch 2 | iter 4260 step 266 | loss train: 0.112, val: 0.152 | iter time: 1187.74 ms\n",
            "Epoch 2 | iter 4270 step 266 | loss train: 0.126, val: 0.152 | iter time: 1201.20 ms\n",
            "Epoch 2 | iter 4280 step 267 | loss train: 0.151, val: 0.152 | iter time: 1916.77 ms\n",
            "Epoch 2 | iter 4290 step 268 | loss train: 0.133, val: 0.152 | iter time: 1913.88 ms\n",
            "Epoch 2 | iter 4300 step 268 | loss train: 0.130, val: 0.152 | iter time: 1178.96 ms\n",
            "Epoch 2 | iter 4310 step 269 | loss train: 0.113, val: 0.152 | iter time: 1237.05 ms\n",
            "Epoch 2 | iter 4320 step 270 | loss train: 0.100, val: 0.152 | iter time: 1239.74 ms (step)\n",
            "Epoch 2 | iter 4330 step 270 | loss train: 0.106, val: 0.152 | iter time: 1237.10 ms\n",
            "Epoch 2 | iter 4340 step 271 | loss train: 0.121, val: 0.152 | iter time: 1235.29 ms\n",
            "Epoch 2 | iter 4350 step 271 | loss train: 0.130, val: 0.152 | iter time: 1236.33 ms\n",
            "Epoch 2 | iter 4360 step 272 | loss train: 0.151, val: 0.152 | iter time: 1915.12 ms\n",
            "Epoch 2 | iter 4370 step 273 | loss train: 0.146, val: 0.152 | iter time: 1627.96 ms\n",
            "Epoch 2 | iter 4380 step 273 | loss train: 0.109, val: 0.152 | iter time: 1219.26 ms\n",
            "Epoch 2 | iter 4390 step 274 | loss train: 0.125, val: 0.152 | iter time: 1721.88 ms\n",
            "Epoch 2 | iter 4400 step 275 | loss train: 0.159, val: 0.152 | iter time: 1883.84 ms (step)\n",
            "Epoch 2 | iter 4410 step 275 | loss train: 0.132, val: 0.152 | iter time: 2401.25 ms\n",
            "Epoch 2 | iter 4420 step 276 | loss train: 0.118, val: 0.152 | iter time: 1192.84 ms\n",
            "Epoch 2 | iter 4430 step 276 | loss train: 0.134, val: 0.152 | iter time: 1915.03 ms\n",
            "Epoch 2 | iter 4440 step 277 | loss train: 0.118, val: 0.152 | iter time: 2654.38 ms\n",
            "Epoch 2 | iter 4450 step 278 | loss train: 0.119, val: 0.152 | iter time: 1424.82 ms\n",
            "Epoch 2 | iter 4460 step 278 | loss train: 0.146, val: 0.152 | iter time: 2007.98 ms\n",
            "Epoch 2 | iter 4470 step 279 | loss train: 0.173, val: 0.152 | iter time: 1237.02 ms\n",
            "Epoch 2 | iter 4480 step 280 | loss train: 0.138, val: 0.152 | iter time: 1193.07 ms (step)\n",
            "Epoch 2 | iter 4490 step 280 | loss train: 0.109, val: 0.152 | iter time: 2639.74 ms\n",
            "Epoch 2 | iter 4500 step 281 | loss train: 0.115, val: 0.152 | iter time: 1192.26 ms\n",
            "Epoch 2 | iter 4510 step 281 | loss train: 0.118, val: 0.152 | iter time: 2062.58 ms\n",
            "Epoch 2 | iter 4520 step 282 | loss train: 0.125, val: 0.152 | iter time: 1190.35 ms\n",
            "Epoch 2 | iter 4530 step 283 | loss train: 0.125, val: 0.152 | iter time: 1425.10 ms\n",
            "Epoch 2 | iter 4540 step 283 | loss train: 0.114, val: 0.152 | iter time: 1186.39 ms\n",
            "Epoch 2 | iter 4550 step 284 | loss train: 0.132, val: 0.152 | iter time: 1211.36 ms\n",
            "Epoch 2 | iter 4560 step 285 | loss train: 0.122, val: 0.152 | iter time: 1197.97 ms (step)\n",
            "Epoch 2 | iter 4570 step 285 | loss train: 0.128, val: 0.152 | iter time: 2009.57 ms\n",
            "Epoch 2 | iter 4580 step 286 | loss train: 0.113, val: 0.152 | iter time: 1192.32 ms\n",
            "Epoch 2 | iter 4590 step 286 | loss train: 0.106, val: 0.152 | iter time: 1234.26 ms\n",
            "Epoch 2 | iter 4600 step 287 | loss train: 0.110, val: 0.152 | iter time: 1190.57 ms\n",
            "Epoch 2 | iter 4610 step 288 | loss train: 0.106, val: 0.152 | iter time: 1211.42 ms\n",
            "Epoch 2 | iter 4620 step 288 | loss train: 0.117, val: 0.152 | iter time: 1236.82 ms\n",
            "Epoch 2 | iter 4630 step 289 | loss train: 0.132, val: 0.152 | iter time: 1755.38 ms\n",
            "Epoch 2 | iter 4640 step 290 | loss train: 0.136, val: 0.152 | iter time: 2129.64 ms (step)\n",
            "Epoch 2 | iter 4650 step 290 | loss train: 0.125, val: 0.152 | iter time: 1189.89 ms\n",
            "Epoch 2 | iter 4660 step 291 | loss train: 0.115, val: 0.152 | iter time: 1892.72 ms\n",
            "Epoch 2 | iter 4670 step 291 | loss train: 0.134, val: 0.152 | iter time: 1914.01 ms\n",
            "Epoch 2 | iter 4680 step 292 | loss train: 0.158, val: 0.152 | iter time: 1191.94 ms\n",
            "Epoch 2 | iter 4690 step 293 | loss train: 0.132, val: 0.152 | iter time: 1438.14 ms\n",
            "Epoch 2 | iter 4700 step 293 | loss train: 0.127, val: 0.152 | iter time: 1187.75 ms\n",
            "Epoch 2 | iter 4710 step 294 | loss train: 0.105, val: 0.152 | iter time: 1178.51 ms\n",
            "Epoch 2 | iter 4720 step 295 | loss train: 0.094, val: 0.152 | iter time: 1096.04 ms (step)\n",
            "Epoch 2 | iter 4730 step 295 | loss train: 0.116, val: 0.152 | iter time: 1237.17 ms\n",
            "Epoch 2 | iter 4740 step 296 | loss train: 0.120, val: 0.152 | iter time: 1725.34 ms\n",
            "Epoch 2 | iter 4750 step 296 | loss train: 0.151, val: 0.152 | iter time: 1756.26 ms\n",
            "Epoch 2 | iter 4760 step 297 | loss train: 0.143, val: 0.152 | iter time: 1237.86 ms\n",
            "Epoch 2 | iter 4770 step 298 | loss train: 0.123, val: 0.152 | iter time: 3349.08 ms\n",
            "Epoch 2 | iter 4780 step 298 | loss train: 0.131, val: 0.152 | iter time: 1422.43 ms\n",
            "Epoch 2 | iter 4790 step 299 | loss train: 0.135, val: 0.152 | iter time: 2008.15 ms\n",
            "Epoch 2 | iter 4800 step 300 | loss train: 0.129, val: 0.152 | iter time: 2064.90 ms (step)\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "Based on your preference for romantic movies, I would recommend the movie \"The Fault in Our Stars\" for you to watch during the weekend. The reason being that it is a heartwarming and moving love story about facing the Below:\n",
            "\n",
            "### Instruction:\n",
            "Generate a response that completes the request.\n",
            "\n",
            "### Response:\n",
            "I'm sorry, but I cannot generate a response as the instruction is incomplete. Please provide the complete request for me to process.\n",
            "iter 4800: val loss 0.1313, val time: 81910.54 ms\n",
            "Epoch 2 | iter 4810 step 300 | loss train: 0.116, val: 0.131 | iter time: 1188.09 ms\n",
            "Epoch 2 | iter 4820 step 301 | loss train: 0.142, val: 0.131 | iter time: 2008.02 ms\n",
            "Epoch 2 | iter 4830 step 301 | loss train: 0.166, val: 0.131 | iter time: 1237.41 ms\n",
            "Epoch 2 | iter 4840 step 302 | loss train: 0.112, val: 0.131 | iter time: 1192.36 ms\n",
            "Epoch 2 | iter 4850 step 303 | loss train: 0.114, val: 0.131 | iter time: 1253.95 ms\n",
            "Epoch 2 | iter 4860 step 303 | loss train: 0.134, val: 0.131 | iter time: 1190.59 ms\n",
            "Epoch 2 | iter 4870 step 304 | loss train: 0.124, val: 0.131 | iter time: 2043.89 ms\n",
            "Epoch 2 | iter 4880 step 305 | loss train: 0.147, val: 0.131 | iter time: 1612.58 ms (step)\n",
            "Epoch 2 | iter 4890 step 305 | loss train: 0.114, val: 0.131 | iter time: 1876.05 ms\n",
            "Epoch 2 | iter 4900 step 306 | loss train: 0.143, val: 0.131 | iter time: 2138.97 ms\n",
            "Epoch 2 | iter 4910 step 306 | loss train: 0.161, val: 0.131 | iter time: 2008.49 ms\n",
            "Epoch 2 | iter 4920 step 307 | loss train: 0.121, val: 0.131 | iter time: 1192.55 ms\n",
            "Epoch 2 | iter 4930 step 308 | loss train: 0.088, val: 0.131 | iter time: 1189.86 ms\n",
            "Epoch 2 | iter 4940 step 308 | loss train: 0.118, val: 0.131 | iter time: 1188.13 ms\n",
            "Epoch 2 | iter 4950 step 309 | loss train: 0.114, val: 0.131 | iter time: 1421.27 ms\n",
            "Epoch 2 | iter 4960 step 310 | loss train: 0.097, val: 0.131 | iter time: 2776.24 ms (step)\n",
            "Epoch 2 | iter 4970 step 310 | loss train: 0.109, val: 0.131 | iter time: 1090.97 ms\n",
            "Epoch 2 | iter 4980 step 311 | loss train: 0.110, val: 0.131 | iter time: 1234.76 ms\n",
            "Epoch 2 | iter 4990 step 311 | loss train: 0.111, val: 0.131 | iter time: 1188.89 ms\n",
            "Epoch 2 | iter 5000 step 312 | loss train: 0.111, val: 0.131 | iter time: 1204.60 ms\n",
            "Epoch 3 | iter 5010 step 313 | loss train: 0.146, val: 0.131 | iter time: 1876.61 ms\n",
            "Epoch 3 | iter 5020 step 313 | loss train: 0.152, val: 0.131 | iter time: 1420.99 ms\n",
            "Epoch 3 | iter 5030 step 314 | loss train: 0.125, val: 0.131 | iter time: 1208.86 ms\n",
            "Epoch 3 | iter 5040 step 315 | loss train: 0.122, val: 0.131 | iter time: 2040.85 ms (step)\n",
            "Epoch 3 | iter 5050 step 315 | loss train: 0.112, val: 0.131 | iter time: 1190.24 ms\n",
            "Epoch 3 | iter 5060 step 316 | loss train: 0.090, val: 0.131 | iter time: 1235.01 ms\n",
            "Epoch 3 | iter 5070 step 316 | loss train: 0.108, val: 0.131 | iter time: 1238.46 ms\n",
            "Epoch 3 | iter 5080 step 317 | loss train: 0.103, val: 0.131 | iter time: 1936.15 ms\n",
            "Epoch 3 | iter 5090 step 318 | loss train: 0.100, val: 0.131 | iter time: 1893.74 ms\n",
            "Epoch 3 | iter 5100 step 318 | loss train: 0.100, val: 0.131 | iter time: 1189.27 ms\n",
            "Epoch 3 | iter 5110 step 319 | loss train: 0.111, val: 0.131 | iter time: 1236.14 ms\n",
            "Epoch 3 | iter 5120 step 320 | loss train: 0.114, val: 0.131 | iter time: 1428.61 ms (step)\n",
            "Epoch 3 | iter 5130 step 320 | loss train: 0.113, val: 0.131 | iter time: 1761.65 ms\n",
            "Epoch 3 | iter 5140 step 321 | loss train: 0.103, val: 0.131 | iter time: 1237.08 ms\n",
            "Epoch 3 | iter 5150 step 321 | loss train: 0.107, val: 0.131 | iter time: 1238.76 ms\n",
            "Epoch 3 | iter 5160 step 322 | loss train: 0.098, val: 0.131 | iter time: 1234.22 ms\n",
            "Epoch 3 | iter 5170 step 323 | loss train: 0.103, val: 0.131 | iter time: 1234.00 ms\n",
            "Epoch 3 | iter 5180 step 323 | loss train: 0.103, val: 0.131 | iter time: 1235.11 ms\n",
            "Epoch 3 | iter 5190 step 324 | loss train: 0.110, val: 0.131 | iter time: 2046.17 ms\n",
            "Epoch 3 | iter 5200 step 325 | loss train: 0.122, val: 0.131 | iter time: 2108.83 ms (step)\n",
            "Epoch 3 | iter 5210 step 325 | loss train: 0.124, val: 0.131 | iter time: 2017.00 ms\n",
            "Epoch 3 | iter 5220 step 326 | loss train: 0.113, val: 0.131 | iter time: 1423.61 ms\n",
            "Epoch 3 | iter 5230 step 326 | loss train: 0.098, val: 0.131 | iter time: 3330.79 ms\n",
            "Epoch 3 | iter 5240 step 327 | loss train: 0.108, val: 0.131 | iter time: 1193.42 ms\n",
            "Epoch 3 | iter 5250 step 328 | loss train: 0.101, val: 0.131 | iter time: 1258.90 ms\n",
            "Epoch 3 | iter 5260 step 328 | loss train: 0.121, val: 0.131 | iter time: 2012.86 ms\n",
            "Epoch 3 | iter 5270 step 329 | loss train: 0.120, val: 0.131 | iter time: 1235.37 ms\n",
            "Epoch 3 | iter 5280 step 330 | loss train: 0.096, val: 0.131 | iter time: 1240.95 ms (step)\n",
            "Epoch 3 | iter 5290 step 330 | loss train: 0.091, val: 0.131 | iter time: 1239.12 ms\n",
            "Epoch 3 | iter 5300 step 331 | loss train: 0.101, val: 0.131 | iter time: 1249.39 ms\n",
            "Epoch 3 | iter 5310 step 331 | loss train: 0.103, val: 0.131 | iter time: 2012.67 ms\n",
            "Epoch 3 | iter 5320 step 332 | loss train: 0.113, val: 0.131 | iter time: 1880.68 ms\n",
            "Epoch 3 | iter 5330 step 333 | loss train: 0.142, val: 0.131 | iter time: 2011.13 ms\n",
            "Epoch 3 | iter 5340 step 333 | loss train: 0.138, val: 0.131 | iter time: 2093.95 ms\n",
            "Epoch 3 | iter 5350 step 334 | loss train: 0.134, val: 0.131 | iter time: 1086.83 ms\n",
            "Epoch 3 | iter 5360 step 335 | loss train: 0.144, val: 0.131 | iter time: 1241.34 ms (step)\n",
            "Epoch 3 | iter 5370 step 335 | loss train: 0.138, val: 0.131 | iter time: 1101.74 ms\n",
            "Epoch 3 | iter 5380 step 336 | loss train: 0.127, val: 0.131 | iter time: 1097.79 ms\n",
            "Epoch 3 | iter 5390 step 336 | loss train: 0.118, val: 0.131 | iter time: 1235.60 ms\n",
            "Epoch 3 | iter 5400 step 337 | loss train: 0.119, val: 0.131 | iter time: 1917.29 ms\n",
            "Epoch 3 | iter 5410 step 338 | loss train: 0.125, val: 0.131 | iter time: 1235.40 ms\n",
            "Epoch 3 | iter 5420 step 338 | loss train: 0.114, val: 0.131 | iter time: 1919.13 ms\n",
            "Epoch 3 | iter 5430 step 339 | loss train: 0.119, val: 0.131 | iter time: 1877.78 ms\n",
            "Epoch 3 | iter 5440 step 340 | loss train: 0.129, val: 0.131 | iter time: 1613.49 ms (step)\n",
            "Epoch 3 | iter 5450 step 340 | loss train: 0.121, val: 0.131 | iter time: 1189.51 ms\n",
            "Epoch 3 | iter 5460 step 341 | loss train: 0.099, val: 0.131 | iter time: 1093.48 ms\n",
            "Epoch 3 | iter 5470 step 341 | loss train: 0.104, val: 0.131 | iter time: 1190.81 ms\n",
            "Epoch 3 | iter 5480 step 342 | loss train: 0.125, val: 0.131 | iter time: 1208.53 ms\n",
            "Epoch 3 | iter 5490 step 343 | loss train: 0.110, val: 0.131 | iter time: 1191.20 ms\n",
            "Epoch 3 | iter 5500 step 343 | loss train: 0.088, val: 0.131 | iter time: 1237.20 ms\n",
            "Epoch 3 | iter 5510 step 344 | loss train: 0.111, val: 0.131 | iter time: 1190.27 ms\n",
            "Epoch 3 | iter 5520 step 345 | loss train: 0.099, val: 0.131 | iter time: 3361.48 ms (step)\n",
            "Epoch 3 | iter 5530 step 345 | loss train: 0.095, val: 0.131 | iter time: 2376.89 ms\n",
            "Epoch 3 | iter 5540 step 346 | loss train: 0.111, val: 0.131 | iter time: 1188.96 ms\n",
            "Epoch 3 | iter 5550 step 346 | loss train: 0.100, val: 0.131 | iter time: 1608.94 ms\n",
            "Epoch 3 | iter 5560 step 347 | loss train: 0.117, val: 0.131 | iter time: 1201.10 ms\n",
            "Epoch 3 | iter 5570 step 348 | loss train: 0.099, val: 0.131 | iter time: 1867.47 ms\n",
            "Epoch 3 | iter 5580 step 348 | loss train: 0.105, val: 0.131 | iter time: 3427.48 ms\n",
            "Epoch 3 | iter 5590 step 349 | loss train: 0.126, val: 0.131 | iter time: 1192.56 ms\n",
            "Epoch 3 | iter 5600 step 350 | loss train: 0.132, val: 0.131 | iter time: 1769.12 ms (step)\n",
            "Epoch 3 | iter 5610 step 350 | loss train: 0.127, val: 0.131 | iter time: 1178.72 ms\n",
            "Epoch 3 | iter 5620 step 351 | loss train: 0.137, val: 0.131 | iter time: 1918.27 ms\n",
            "Epoch 3 | iter 5630 step 351 | loss train: 0.117, val: 0.131 | iter time: 1188.68 ms\n",
            "Epoch 3 | iter 5640 step 352 | loss train: 0.113, val: 0.131 | iter time: 1192.43 ms\n",
            "Epoch 3 | iter 5650 step 353 | loss train: 0.139, val: 0.131 | iter time: 1914.42 ms\n",
            "Epoch 3 | iter 5660 step 353 | loss train: 0.115, val: 0.131 | iter time: 1187.35 ms\n",
            "Epoch 3 | iter 5670 step 354 | loss train: 0.093, val: 0.131 | iter time: 1194.83 ms\n",
            "Epoch 3 | iter 5680 step 355 | loss train: 0.115, val: 0.131 | iter time: 2491.13 ms (step)\n",
            "Epoch 3 | iter 5690 step 355 | loss train: 0.109, val: 0.131 | iter time: 1193.89 ms\n",
            "Epoch 3 | iter 5700 step 356 | loss train: 0.110, val: 0.131 | iter time: 2349.58 ms\n",
            "Epoch 3 | iter 5710 step 356 | loss train: 0.095, val: 0.131 | iter time: 3325.27 ms\n",
            "Epoch 3 | iter 5720 step 357 | loss train: 0.107, val: 0.131 | iter time: 1428.96 ms\n",
            "Epoch 3 | iter 5730 step 358 | loss train: 0.124, val: 0.131 | iter time: 1914.90 ms\n",
            "Epoch 3 | iter 5740 step 358 | loss train: 0.115, val: 0.131 | iter time: 1190.41 ms\n",
            "Epoch 3 | iter 5750 step 359 | loss train: 0.114, val: 0.131 | iter time: 3346.29 ms\n",
            "Epoch 3 | iter 5760 step 360 | loss train: 0.099, val: 0.131 | iter time: 1445.13 ms (step)\n",
            "Epoch 3 | iter 5770 step 360 | loss train: 0.106, val: 0.131 | iter time: 1195.63 ms\n",
            "Epoch 3 | iter 5780 step 361 | loss train: 0.099, val: 0.131 | iter time: 2758.96 ms\n",
            "Epoch 3 | iter 5790 step 361 | loss train: 0.115, val: 0.131 | iter time: 2099.46 ms\n",
            "Epoch 3 | iter 5800 step 362 | loss train: 0.126, val: 0.131 | iter time: 1213.90 ms\n",
            "Epoch 3 | iter 5810 step 363 | loss train: 0.116, val: 0.131 | iter time: 1191.94 ms\n",
            "Epoch 3 | iter 5820 step 363 | loss train: 0.109, val: 0.131 | iter time: 1194.29 ms\n",
            "Epoch 3 | iter 5830 step 364 | loss train: 0.090, val: 0.131 | iter time: 1235.75 ms\n",
            "Epoch 3 | iter 5840 step 365 | loss train: 0.097, val: 0.131 | iter time: 1219.47 ms (step)\n",
            "Epoch 3 | iter 5850 step 365 | loss train: 0.105, val: 0.131 | iter time: 1191.44 ms\n",
            "Epoch 3 | iter 5860 step 366 | loss train: 0.122, val: 0.131 | iter time: 1189.11 ms\n",
            "Epoch 3 | iter 5870 step 366 | loss train: 0.120, val: 0.131 | iter time: 1237.90 ms\n",
            "Epoch 3 | iter 5880 step 367 | loss train: 0.090, val: 0.131 | iter time: 1199.05 ms\n",
            "Epoch 3 | iter 5890 step 368 | loss train: 0.102, val: 0.131 | iter time: 1114.83 ms\n",
            "Epoch 3 | iter 5900 step 368 | loss train: 0.116, val: 0.131 | iter time: 1238.74 ms\n",
            "Epoch 3 | iter 5910 step 369 | loss train: 0.121, val: 0.131 | iter time: 1206.46 ms\n",
            "Epoch 3 | iter 5920 step 370 | loss train: 0.119, val: 0.131 | iter time: 1880.27 ms (step)\n",
            "Epoch 3 | iter 5930 step 370 | loss train: 0.122, val: 0.131 | iter time: 1216.93 ms\n",
            "Epoch 3 | iter 5940 step 371 | loss train: 0.137, val: 0.131 | iter time: 1192.46 ms\n",
            "Epoch 3 | iter 5950 step 371 | loss train: 0.115, val: 0.131 | iter time: 1879.77 ms\n",
            "Epoch 3 | iter 5960 step 372 | loss train: 0.110, val: 0.131 | iter time: 1191.58 ms\n",
            "Epoch 3 | iter 5970 step 373 | loss train: 0.122, val: 0.131 | iter time: 1191.97 ms\n",
            "Epoch 3 | iter 5980 step 373 | loss train: 0.122, val: 0.131 | iter time: 1612.02 ms\n",
            "Epoch 3 | iter 5990 step 374 | loss train: 0.104, val: 0.131 | iter time: 1426.80 ms\n",
            "Epoch 3 | iter 6000 step 375 | loss train: 0.107, val: 0.131 | iter time: 1944.66 ms (step)\n",
            "Epoch 3 | iter 6010 step 375 | loss train: 0.144, val: 0.131 | iter time: 2039.59 ms\n",
            "Epoch 3 | iter 6020 step 376 | loss train: 0.130, val: 0.131 | iter time: 1720.03 ms\n",
            "Epoch 3 | iter 6030 step 376 | loss train: 0.116, val: 0.131 | iter time: 1206.99 ms\n",
            "Epoch 3 | iter 6040 step 377 | loss train: 0.120, val: 0.131 | iter time: 1236.42 ms\n",
            "Epoch 3 | iter 6050 step 378 | loss train: 0.104, val: 0.131 | iter time: 1193.46 ms\n",
            "Epoch 3 | iter 6060 step 378 | loss train: 0.094, val: 0.131 | iter time: 1235.58 ms\n",
            "Epoch 3 | iter 6070 step 379 | loss train: 0.130, val: 0.131 | iter time: 1192.44 ms\n",
            "Epoch 3 | iter 6080 step 380 | loss train: 0.126, val: 0.131 | iter time: 2014.78 ms (step)\n",
            "Epoch 3 | iter 6090 step 380 | loss train: 0.138, val: 0.131 | iter time: 1421.54 ms\n",
            "Epoch 3 | iter 6100 step 381 | loss train: 0.112, val: 0.131 | iter time: 1191.07 ms\n",
            "Epoch 3 | iter 6110 step 381 | loss train: 0.106, val: 0.131 | iter time: 1215.03 ms\n",
            "Epoch 3 | iter 6120 step 382 | loss train: 0.120, val: 0.131 | iter time: 2088.45 ms\n",
            "Epoch 3 | iter 6130 step 383 | loss train: 0.123, val: 0.131 | iter time: 1240.29 ms\n",
            "Epoch 3 | iter 6140 step 383 | loss train: 0.088, val: 0.131 | iter time: 1193.19 ms\n",
            "Epoch 3 | iter 6150 step 384 | loss train: 0.101, val: 0.131 | iter time: 1238.37 ms\n",
            "Epoch 3 | iter 6160 step 385 | loss train: 0.107, val: 0.131 | iter time: 1194.95 ms (step)\n",
            "Epoch 3 | iter 6170 step 385 | loss train: 0.096, val: 0.131 | iter time: 1237.20 ms\n",
            "Epoch 3 | iter 6180 step 386 | loss train: 0.106, val: 0.131 | iter time: 1195.80 ms\n",
            "Epoch 3 | iter 6190 step 386 | loss train: 0.117, val: 0.131 | iter time: 1192.90 ms\n",
            "Epoch 3 | iter 6200 step 387 | loss train: 0.123, val: 0.131 | iter time: 3331.09 ms\n",
            "Epoch 3 | iter 6210 step 388 | loss train: 0.101, val: 0.131 | iter time: 2115.30 ms\n",
            "Epoch 3 | iter 6220 step 388 | loss train: 0.115, val: 0.131 | iter time: 1237.68 ms\n",
            "Epoch 3 | iter 6230 step 389 | loss train: 0.121, val: 0.131 | iter time: 2009.80 ms\n",
            "Epoch 3 | iter 6240 step 390 | loss train: 0.124, val: 0.131 | iter time: 1448.42 ms (step)\n",
            "Epoch 3 | iter 6250 step 390 | loss train: 0.117, val: 0.131 | iter time: 1233.91 ms\n",
            "Epoch 3 | iter 6260 step 391 | loss train: 0.139, val: 0.131 | iter time: 1608.72 ms\n",
            "Epoch 3 | iter 6270 step 391 | loss train: 0.115, val: 0.131 | iter time: 1236.50 ms\n",
            "Epoch 3 | iter 6280 step 392 | loss train: 0.112, val: 0.131 | iter time: 1713.89 ms\n",
            "Epoch 3 | iter 6290 step 393 | loss train: 0.112, val: 0.131 | iter time: 1876.56 ms\n",
            "Epoch 3 | iter 6300 step 393 | loss train: 0.109, val: 0.131 | iter time: 1191.75 ms\n",
            "Epoch 3 | iter 6310 step 394 | loss train: 0.100, val: 0.131 | iter time: 1253.16 ms\n",
            "Epoch 3 | iter 6320 step 395 | loss train: 0.103, val: 0.131 | iter time: 1892.84 ms (step)\n",
            "Epoch 3 | iter 6330 step 395 | loss train: 0.124, val: 0.131 | iter time: 1423.65 ms\n",
            "Epoch 3 | iter 6340 step 396 | loss train: 0.123, val: 0.131 | iter time: 1235.12 ms\n",
            "Epoch 3 | iter 6350 step 396 | loss train: 0.093, val: 0.131 | iter time: 1236.61 ms\n",
            "Epoch 3 | iter 6360 step 397 | loss train: 0.106, val: 0.131 | iter time: 2010.94 ms\n",
            "Epoch 3 | iter 6370 step 398 | loss train: 0.115, val: 0.131 | iter time: 1236.76 ms\n",
            "Epoch 3 | iter 6380 step 398 | loss train: 0.120, val: 0.131 | iter time: 1443.50 ms\n",
            "Epoch 3 | iter 6390 step 399 | loss train: 0.102, val: 0.131 | iter time: 1255.83 ms\n",
            "Epoch 3 | iter 6400 step 400 | loss train: 0.118, val: 0.131 | iter time: 1922.66 ms (step)\n",
            "Validating ...\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Recommend a movie for me to watch during the weekend and explain the reason.\n",
            "\n",
            "### Response:\n",
            "Based on your preference for dramas and mysteries, I would recommend you watch \"Gone Girl\" during the weekend. This movie is a psychological thriller that will leave you guessing until the end.\n",
            "iter 6400: val loss 0.1193, val time: 77638.14 ms\n",
            "Epoch 3 | iter 6410 step 400 | loss train: 0.155, val: 0.119 | iter time: 2096.57 ms\n",
            "Epoch 3 | iter 6420 step 401 | loss train: 0.126, val: 0.119 | iter time: 1207.17 ms\n",
            "Epoch 3 | iter 6430 step 401 | loss train: 0.109, val: 0.119 | iter time: 1239.49 ms\n",
            "Epoch 3 | iter 6440 step 402 | loss train: 0.105, val: 0.119 | iter time: 1236.00 ms\n",
            "Epoch 3 | iter 6450 step 403 | loss train: 0.110, val: 0.119 | iter time: 1880.69 ms\n",
            "Epoch 3 | iter 6460 step 403 | loss train: 0.088, val: 0.119 | iter time: 1246.64 ms\n",
            "Epoch 3 | iter 6470 step 404 | loss train: 0.104, val: 0.119 | iter time: 2644.34 ms\n",
            "Epoch 3 | iter 6480 step 405 | loss train: 0.116, val: 0.119 | iter time: 1196.56 ms (step)\n",
            "Epoch 3 | iter 6490 step 405 | loss train: 0.110, val: 0.119 | iter time: 1236.11 ms\n",
            "Epoch 3 | iter 6500 step 406 | loss train: 0.107, val: 0.119 | iter time: 1194.10 ms\n",
            "Epoch 3 | iter 6510 step 406 | loss train: 0.121, val: 0.119 | iter time: 1916.75 ms\n",
            "Epoch 3 | iter 6520 step 407 | loss train: 0.119, val: 0.119 | iter time: 2033.35 ms\n",
            "Epoch 3 | iter 6530 step 408 | loss train: 0.118, val: 0.119 | iter time: 1437.01 ms\n",
            "Epoch 3 | iter 6540 step 408 | loss train: 0.101, val: 0.119 | iter time: 1091.62 ms\n",
            "Epoch 3 | iter 6550 step 409 | loss train: 0.105, val: 0.119 | iter time: 2036.78 ms\n",
            "Epoch 3 | iter 6560 step 410 | loss train: 0.113, val: 0.119 | iter time: 1197.49 ms (step)\n",
            "Epoch 3 | iter 6570 step 410 | loss train: 0.110, val: 0.119 | iter time: 1940.39 ms\n",
            "Epoch 3 | iter 6580 step 411 | loss train: 0.113, val: 0.119 | iter time: 1191.84 ms\n",
            "Epoch 3 | iter 6590 step 411 | loss train: 0.108, val: 0.119 | iter time: 1420.80 ms\n",
            "Epoch 3 | iter 6600 step 412 | loss train: 0.112, val: 0.119 | iter time: 1610.36 ms\n",
            "Epoch 3 | iter 6610 step 413 | loss train: 0.118, val: 0.119 | iter time: 1237.70 ms\n",
            "Epoch 3 | iter 6620 step 413 | loss train: 0.104, val: 0.119 | iter time: 1761.07 ms\n",
            "Epoch 3 | iter 6630 step 414 | loss train: 0.112, val: 0.119 | iter time: 2640.23 ms\n",
            "Epoch 3 | iter 6640 step 415 | loss train: 0.116, val: 0.119 | iter time: 2657.40 ms (step)\n",
            "Epoch 3 | iter 6650 step 415 | loss train: 0.099, val: 0.119 | iter time: 1215.51 ms\n",
            "Epoch 3 | iter 6660 step 416 | loss train: 0.124, val: 0.119 | iter time: 2006.42 ms\n",
            "Epoch 3 | iter 6670 step 416 | loss train: 0.123, val: 0.119 | iter time: 1756.41 ms\n",
            "Epoch 3 | iter 6680 step 417 | loss train: 0.106, val: 0.119 | iter time: 1236.26 ms\n",
            "Epoch 3 | iter 6690 step 418 | loss train: 0.113, val: 0.119 | iter time: 1189.73 ms\n",
            "Epoch 3 | iter 6700 step 418 | loss train: 0.110, val: 0.119 | iter time: 1190.32 ms\n",
            "Epoch 3 | iter 6710 step 419 | loss train: 0.118, val: 0.119 | iter time: 1911.60 ms\n",
            "Epoch 3 | iter 6720 step 420 | loss train: 0.134, val: 0.119 | iter time: 1095.87 ms (step)\n",
            "Epoch 3 | iter 6730 step 420 | loss train: 0.127, val: 0.119 | iter time: 1208.42 ms\n",
            "Epoch 3 | iter 6740 step 421 | loss train: 0.117, val: 0.119 | iter time: 1256.43 ms\n",
            "Epoch 3 | iter 6750 step 421 | loss train: 0.101, val: 0.119 | iter time: 1236.61 ms\n",
            "Epoch 3 | iter 6760 step 422 | loss train: 0.097, val: 0.119 | iter time: 1237.82 ms\n",
            "Epoch 3 | iter 6770 step 423 | loss train: 0.103, val: 0.119 | iter time: 2219.25 ms\n",
            "Epoch 3 | iter 6780 step 423 | loss train: 0.115, val: 0.119 | iter time: 1192.51 ms\n",
            "Epoch 3 | iter 6790 step 424 | loss train: 0.108, val: 0.119 | iter time: 2775.92 ms\n",
            "Epoch 3 | iter 6800 step 425 | loss train: 0.100, val: 0.119 | iter time: 1198.31 ms (step)\n",
            "Epoch 3 | iter 6810 step 425 | loss train: 0.098, val: 0.119 | iter time: 1089.88 ms\n",
            "Epoch 3 | iter 6820 step 426 | loss train: 0.111, val: 0.119 | iter time: 1420.21 ms\n",
            "Epoch 3 | iter 6830 step 426 | loss train: 0.114, val: 0.119 | iter time: 1193.07 ms\n",
            "Epoch 3 | iter 6840 step 427 | loss train: 0.108, val: 0.119 | iter time: 1422.90 ms\n",
            "Epoch 3 | iter 6850 step 428 | loss train: 0.109, val: 0.119 | iter time: 1931.81 ms\n",
            "Epoch 3 | iter 6860 step 428 | loss train: 0.117, val: 0.119 | iter time: 1879.45 ms\n",
            "Epoch 3 | iter 6870 step 429 | loss train: 0.119, val: 0.119 | iter time: 1191.06 ms\n",
            "Epoch 3 | iter 6880 step 430 | loss train: 0.117, val: 0.119 | iter time: 1187.26 ms (step)\n",
            "Epoch 3 | iter 6890 step 430 | loss train: 0.090, val: 0.119 | iter time: 1195.09 ms\n",
            "Epoch 3 | iter 6900 step 431 | loss train: 0.112, val: 0.119 | iter time: 2011.12 ms\n",
            "Epoch 3 | iter 6910 step 431 | loss train: 0.121, val: 0.119 | iter time: 1426.96 ms\n",
            "Epoch 3 | iter 6920 step 432 | loss train: 0.123, val: 0.119 | iter time: 1090.73 ms\n",
            "Epoch 3 | iter 6930 step 433 | loss train: 0.113, val: 0.119 | iter time: 1194.83 ms\n",
            "Epoch 3 | iter 6940 step 433 | loss train: 0.105, val: 0.119 | iter time: 1915.66 ms\n",
            "Epoch 3 | iter 6950 step 434 | loss train: 0.113, val: 0.119 | iter time: 1192.37 ms\n",
            "Epoch 3 | iter 6960 step 435 | loss train: 0.105, val: 0.119 | iter time: 1240.10 ms (step)\n",
            "Epoch 3 | iter 6970 step 435 | loss train: 0.106, val: 0.119 | iter time: 1997.41 ms\n",
            "Epoch 3 | iter 6980 step 436 | loss train: 0.115, val: 0.119 | iter time: 1614.26 ms\n",
            "Epoch 3 | iter 6990 step 436 | loss train: 0.101, val: 0.119 | iter time: 2034.89 ms\n",
            "Epoch 3 | iter 7000 step 437 | loss train: 0.131, val: 0.119 | iter time: 1613.72 ms\n",
            "Epoch 3 | iter 7010 step 438 | loss train: 0.113, val: 0.119 | iter time: 1238.04 ms\n",
            "Epoch 3 | iter 7020 step 438 | loss train: 0.111, val: 0.119 | iter time: 1189.58 ms\n",
            "Epoch 3 | iter 7030 step 439 | loss train: 0.115, val: 0.119 | iter time: 1878.52 ms\n",
            "Epoch 3 | iter 7040 step 440 | loss train: 0.116, val: 0.119 | iter time: 1225.93 ms (step)\n",
            "Epoch 3 | iter 7050 step 440 | loss train: 0.106, val: 0.119 | iter time: 1916.82 ms\n",
            "Epoch 3 | iter 7060 step 441 | loss train: 0.139, val: 0.119 | iter time: 1880.25 ms\n",
            "Epoch 3 | iter 7070 step 441 | loss train: 0.113, val: 0.119 | iter time: 1424.33 ms\n",
            "Epoch 3 | iter 7080 step 442 | loss train: 0.089, val: 0.119 | iter time: 1248.98 ms\n",
            "Epoch 3 | iter 7090 step 443 | loss train: 0.094, val: 0.119 | iter time: 1090.89 ms\n",
            "Epoch 3 | iter 7100 step 443 | loss train: 0.090, val: 0.119 | iter time: 3290.93 ms\n",
            "Epoch 3 | iter 7110 step 444 | loss train: 0.095, val: 0.119 | iter time: 1239.27 ms\n",
            "Epoch 3 | iter 7120 step 445 | loss train: 0.093, val: 0.119 | iter time: 2493.65 ms (step)\n",
            "Epoch 3 | iter 7130 step 445 | loss train: 0.089, val: 0.119 | iter time: 1234.31 ms\n",
            "Epoch 3 | iter 7140 step 446 | loss train: 0.105, val: 0.119 | iter time: 1421.16 ms\n",
            "Epoch 3 | iter 7150 step 446 | loss train: 0.108, val: 0.119 | iter time: 1212.59 ms\n",
            "Epoch 3 | iter 7160 step 447 | loss train: 0.101, val: 0.119 | iter time: 1261.67 ms\n",
            "Epoch 3 | iter 7170 step 448 | loss train: 0.111, val: 0.119 | iter time: 1190.91 ms\n",
            "Epoch 3 | iter 7180 step 448 | loss train: 0.095, val: 0.119 | iter time: 1239.57 ms\n",
            "Epoch 3 | iter 7190 step 449 | loss train: 0.086, val: 0.119 | iter time: 1422.88 ms\n",
            "Epoch 3 | iter 7200 step 450 | loss train: 0.105, val: 0.119 | iter time: 1196.99 ms (step)\n",
            "Epoch 3 | iter 7210 step 450 | loss train: 0.116, val: 0.119 | iter time: 1193.05 ms\n",
            "Epoch 3 | iter 7220 step 451 | loss train: 0.115, val: 0.119 | iter time: 1089.44 ms\n",
            "Epoch 3 | iter 7230 step 451 | loss train: 0.107, val: 0.119 | iter time: 1207.11 ms\n",
            "Epoch 3 | iter 7240 step 452 | loss train: 0.097, val: 0.119 | iter time: 1239.21 ms\n",
            "Epoch 3 | iter 7250 step 453 | loss train: 0.096, val: 0.119 | iter time: 2645.88 ms\n",
            "Epoch 3 | iter 7260 step 453 | loss train: 0.089, val: 0.119 | iter time: 1084.94 ms\n",
            "Epoch 3 | iter 7270 step 454 | loss train: 0.100, val: 0.119 | iter time: 1211.84 ms\n",
            "Epoch 3 | iter 7280 step 455 | loss train: 0.115, val: 0.119 | iter time: 1242.17 ms (step)\n",
            "Epoch 3 | iter 7290 step 455 | loss train: 0.103, val: 0.119 | iter time: 1234.76 ms\n",
            "Epoch 3 | iter 7300 step 456 | loss train: 0.106, val: 0.119 | iter time: 1881.19 ms\n",
            "Epoch 3 | iter 7310 step 456 | loss train: 0.131, val: 0.119 | iter time: 1423.49 ms\n",
            "Epoch 3 | iter 7320 step 457 | loss train: 0.121, val: 0.119 | iter time: 2051.19 ms\n",
            "Epoch 3 | iter 7330 step 458 | loss train: 0.128, val: 0.119 | iter time: 1190.98 ms\n",
            "Epoch 3 | iter 7340 step 458 | loss train: 0.113, val: 0.119 | iter time: 1192.60 ms\n",
            "Epoch 3 | iter 7350 step 459 | loss train: 0.112, val: 0.119 | iter time: 2040.15 ms\n",
            "Epoch 3 | iter 7360 step 460 | loss train: 0.108, val: 0.119 | iter time: 1195.50 ms (step)\n",
            "Epoch 3 | iter 7370 step 460 | loss train: 0.106, val: 0.119 | iter time: 1191.13 ms\n",
            "Epoch 3 | iter 7380 step 461 | loss train: 0.105, val: 0.119 | iter time: 1192.32 ms\n",
            "Epoch 3 | iter 7390 step 461 | loss train: 0.102, val: 0.119 | iter time: 1235.69 ms\n",
            "Epoch 3 | iter 7400 step 462 | loss train: 0.106, val: 0.119 | iter time: 2376.53 ms\n",
            "Epoch 3 | iter 7410 step 463 | loss train: 0.085, val: 0.119 | iter time: 1240.26 ms\n",
            "Epoch 3 | iter 7420 step 463 | loss train: 0.096, val: 0.119 | iter time: 1191.23 ms\n",
            "Epoch 3 | iter 7430 step 464 | loss train: 0.089, val: 0.119 | iter time: 1237.01 ms\n",
            "Epoch 3 | iter 7440 step 465 | loss train: 0.100, val: 0.119 | iter time: 1215.10 ms (step)\n",
            "Epoch 3 | iter 7450 step 465 | loss train: 0.115, val: 0.119 | iter time: 1891.38 ms\n",
            "Epoch 3 | iter 7460 step 466 | loss train: 0.106, val: 0.119 | iter time: 1236.31 ms\n",
            "Epoch 3 | iter 7470 step 466 | loss train: 0.114, val: 0.119 | iter time: 1193.74 ms\n",
            "Epoch 3 | iter 7480 step 467 | loss train: 0.103, val: 0.119 | iter time: 1195.27 ms\n",
            "Epoch 3 | iter 7490 step 468 | loss train: 0.099, val: 0.119 | iter time: 1179.75 ms\n",
            "Epoch 3 | iter 7500 step 468 | loss train: 0.120, val: 0.119 | iter time: 1424.35 ms\n",
            "Training time: 11566.72s\n",
            "Memory used: 15.17 GB\n",
            "Saving LoRA weights to 'out4/lora_weights/mistral7B-finetuned/final/lit_model.pth.lora'\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt generate base \\\n",
        "  --checkpoint_dir \"out4/lora_weights/mistral7B-finetuned/final\" \\\n",
        "  --prompt \"Rohan is shorter than Seema. Krishna is taller than Pushpa but shorter than Anand. Dhiraj is taller than Krishna but shorter than Seema. Dhiraj is taller than Rohan. Question: Is Dhiraj the tallest ? Based on context and question infer entailment or non-entailment\" \\\n",
        "  --precision bf16-true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cxTYKwbi87L",
        "outputId": "f0d78648-c2ce-4230-ede8-cd56ec5894be"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'out4/lora_weights/mistral7B-finetuned/final/lit_model.pth' with {'name': 'Mistral-7B-Instruct-v0.2', 'hf_config': {'name': 'Mistral-7B-Instruct-v0.2', 'org': 'mistralai'}, 'scale_embeddings': False, 'block_size': 32768, 'vocab_size': 32000, 'padding_multiple': 512, 'padded_vocab_size': 32000, 'n_layer': 32, 'n_head': 32, 'head_size': 128, 'n_embd': 4096, 'rotary_percentage': 1.0, 'parallel_residual': False, 'bias': False, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, 'norm_class_name': 'RMSNorm', 'norm_eps': 1e-05, 'mlp_class_name': 'LLaMAMLP', 'gelu_approximate': 'none', 'intermediate_size': 14336, 'rope_condense_ratio': 1, 'rope_base': 10000, 'n_expert': 0, 'n_expert_per_token': 0, 'rope_n_elem': 128}\n",
            "Time to instantiate model: 0.30 seconds.\n",
            "Time to load the model weights: 83.19 seconds.\n",
            "Seed set to 1234\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rohan is shorter than Seema. Krishna is taller than Pushpa but shorter than Anand. Dhiraj is taller than Krishna but shorter than Seema. Dhiraj is taller than Rohan. Question: Is Dhiraj the tallest ? Based on context and question infer entailment or non-entailment\n",
            "\n",
            "### Response:\n",
            "not entailment - contradiction\n",
            "\n",
            "Time for inference 1: 1.83 sec total, 4.91 tokens/sec\n",
            "Memory used: 14.60 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging LoRA Weights"
      ],
      "metadata": {
        "id": "spmrBqQGi3_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!litgpt merge_lora \\\n",
        " --checkpoint_dir \"out4/lora_weights/mistral7B-finetuned/final/\" \\\n",
        " --pretrained_checkpoint_dir \"checkpoints/mistralai/Mistral-7B-Instruct-v0.2/\" \\\n",
        " --precision bf16-true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOuhMfoQjaZr",
        "outputId": "f3a3f966-1896-4908-8f2a-5f7d5685b914"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/litgpt.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e47CMs2NjcEZ",
        "outputId": "bcb4ad8e-7190-47b2-d21c-5d1eaf860aa6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'litgpt'...\n",
            "remote: Enumerating objects: 7960, done.\u001b[K\n",
            "remote: Counting objects: 100% (3211/3211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1108/1108), done.\u001b[K\n",
            "remote: Total 7960 (delta 2603), reused 2321 (delta 2092), pack-reused 4749\u001b[K\n",
            "Receiving objects: 100% (7960/7960), 3.61 MiB | 16.30 MiB/s, done.\n",
            "Resolving deltas: 100% (5617/5617), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "import yaml\n",
        "\n",
        "from litgpt.lora import GPT, Config, lora_filter, merge_lora_weights\n",
        "from litgpt.utils import CLI, check_valid_checkpoint_dir, lazy_load"
      ],
      "metadata": {
        "id": "epKEdWWvcoM6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lora_metadata(checkpoint_dir: Path) -> Tuple[Dict[str, Any], Path, Optional[str]]:\n",
        "    hparams_file = checkpoint_dir / \"hyperparameters.yaml\"\n",
        "    if not hparams_file.is_file():\n",
        "        raise FileNotFoundError(\n",
        "            f\"The path {str(hparams_file)!r} is not a valid checkpoint directory. It is missing a\"\n",
        "            f\" `hyperparameters.yaml` file. Please point to the checkpoint directory that was produced by\"\n",
        "            f\" the `litgpt/finetune/lora.py` script.\"\n",
        "        )\n",
        "\n",
        "    with open(hparams_file, \"r\") as file:\n",
        "        hparams = yaml.safe_load(file)\n",
        "\n",
        "    lora_params = {k: v for k, v in hparams.items() if k.startswith(\"lora_\")}\n",
        "    pretrained_checkpoint_dir = Path(hparams[\"checkpoint_dir\"])\n",
        "    precision = hparams.get(\"precision\")\n",
        "    return lora_params, pretrained_checkpoint_dir, precision"
      ],
      "metadata": {
        "id": "WXWQSgAPm-FH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_lora(\n",
        "    checkpoint_dir: Path, pretrained_checkpoint_dir: Optional[Path] = None, precision: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"Merges the LoRA weights with the base model. See ``litgpt finetune lora``.\n",
        "\n",
        "    Creates a new ``lit_model.pth`` file by merging the LoRA weights (``lit_model.pth.lora``)\n",
        "    with the original checkpoint weights.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_dir: Path to the checkpoint directory with trained LoRA weights, which is the output of\n",
        "            ``litgpt finetune --method lora``.\n",
        "        pretrained_checkpoint_dir: Optional path to the checkpoint directory with the weights of the base model\n",
        "            corresponding to the LoRA checkpoint. By default, this will automatically be inferred from the metadata\n",
        "            in the given `checkpoint_dir` directory. Only set this if the base model's checkpoint directory\n",
        "            has moved or was renamed.\n",
        "        precision: Optional precision setting to instantiate the model weights in. By default, this will\n",
        "            automatically be inferred from the metadata in the given ``checkpoint_dir`` directory.\n",
        "    \"\"\"\n",
        "    check_valid_checkpoint_dir(checkpoint_dir, lora=True)\n",
        "    if pretrained_checkpoint_dir is not None:\n",
        "        check_valid_checkpoint_dir(pretrained_checkpoint_dir)\n",
        "    if (checkpoint_dir / \"lit_model.pth\").is_file():\n",
        "        print(\"LoRA weights have already been merged in this checkpoint.\")\n",
        "        return\n",
        "\n",
        "    lora_params, pretrained_checkpoint_dir, lora_precision = load_lora_metadata(checkpoint_dir)\n",
        "    precision = precision if precision is not None else lora_precision\n",
        "\n",
        "    fabric = L.Fabric(devices=1, precision=precision, accelerator=\"cuda\")\n",
        "    config = Config.from_file(checkpoint_dir / \"model_config.yaml\", **lora_params)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "\n",
        "    lora_path = checkpoint_dir / \"lit_model.pth.lora\"\n",
        "    pretrained_checkpoint = lazy_load(pretrained_checkpoint_dir / \"lit_model.pth\")\n",
        "    lora_checkpoint = lazy_load(lora_path)\n",
        "\n",
        "    # Merge LoRA weights into the base model\n",
        "    pretrained_checkpoint.update(lora_checkpoint.get(\"model\", lora_checkpoint))\n",
        "    model.load_state_dict(pretrained_checkpoint)\n",
        "    merge_lora_weights(model)\n",
        "\n",
        "    # Remove LoRA parameters and the LoRA linear substring\n",
        "    state_dict = {k.replace(\"linear.\", \"\"): v for k, v in model.state_dict().items() if not lora_filter(k, v)}\n",
        "    save_path = checkpoint_dir / \"lit_model.pth\"\n",
        "    torch.save(state_dict, save_path)\n",
        "\n",
        "    fabric.print(f\"Saved merged weights to {str(checkpoint_dir / 'lit_model.pth')!r}\")\n"
      ],
      "metadata": {
        "id": "cfZ-iedvi0uz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merge_lora(\n",
        "    checkpoint_dir=Path(\"out4/lora_weights/mistral7B-finetuned/final/\"),\n",
        "    pretrained_checkpoint_dir=Path(\"checkpoints/mistralai/Mistral-7B-Instruct-v0.2/\"),\n",
        "    precision=\"bf16-true\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HikrQUUDnXBk",
        "outputId": "05f122ae-958c-4964-fd4f-f8a7b7fabec0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved merged weights to 'out4/lora_weights/mistral7B-finetuned/final/lit_model.pth'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "-h5OzzBKs5u7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zip and Save"
      ],
      "metadata": {
        "id": "revbUaVn6Mo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r out4/lora_weights/mistral7B-finetuned-logic-final.zip out4/lora_weights/mistral7B-finetuned/final/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl3rrAc5plIa",
        "outputId": "b8cda3a2-e354-48e9-d1d8-6008946f696c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: out4/lora_weights/mistral7B-finetuned/final/ (stored 0%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/tokenizer_config.json (deflated 64%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/lit_model.pth (deflated 21%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/model_config.yaml (deflated 43%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/lit_model.pth.lora (deflated 21%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/prompt_style.yaml (stored 0%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/generation_config.json (deflated 20%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/hyperparameters.yaml (deflated 45%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/tokenizer.json (deflated 74%)\n",
            "  adding: out4/lora_weights/mistral7B-finetuned/final/tokenizer.model (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp out4/lora_weights/mistral7B-finetuned-logic-final.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "rPWwL7kX1_8Y"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}